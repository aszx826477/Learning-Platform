{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "sc.addPyFile(\"graphframes-0.7.0-spark2.4-s_2.11.jar\")\n",
    "\n",
    "from graphframes import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quizz 1 RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 1 \n",
    "Find the most frequent word. Output this word and its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "lines = sc.textFile('data/README.md')\n",
    "counts = lines.flatMap(lambda x: x.split()) \\\n",
    "              .map(lambda x: (x, 1)) \\\n",
    "              .reduceByKey(add)\n",
    "\n",
    "\n",
    "# Solution 1:\n",
    "counts.max(lambda x: x[1])\n",
    "\n",
    "# Solution 2:\n",
    "#counts.sortBy(lambda x: x[1], ascending=False).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 2\n",
    "Modify the word count example above, so that we only count the frequencies of those words consisting of 5 or more characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "lines = sc.textFile('README.md')\n",
    "counts = lines.flatMap(lambda x: x.split()) \\\n",
    "              .filter(lambda z: len(z) >= 5) \\\n",
    "              .map(lambda x: (x, 1)) \\\n",
    "              .reduceByKey(add)\n",
    "print(counts.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 3\n",
    "Consider the following piece of code:\n",
    "```\n",
    "A = sc.parallelize(xrange(1, 100))\n",
    "t = 50\n",
    "B = A.filter(lambda x: x < t)\n",
    "print B.count()\n",
    "t = 10\n",
    "C = B.filter(lambda x: x > t)\n",
    "print C.count()\n",
    "```\n",
    "What's its output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sc.parallelize(xrange(1, 100))\n",
    "t = 50\n",
    "B = A.filter(lambda x: x < t)\n",
    "print B.count()\n",
    "t = 10\n",
    "C = B.filter(lambda x: x > t)\n",
    "print C.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 4\n",
    "The intent of the code above is to get all numbers below 50 from A and put them into B, and then get all numbers above 10 from B and put them into C.  Fix the code so that it produces the desired behavior, by adding one line of code.  You are not allowed to change the existing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sc.parallelize(range(1, 100))\n",
    "t = 50\n",
    "B = A.filter(lambda x: x < t)\n",
    "print (B.count())\n",
    "B.cache() # Add this line to fix the bug\n",
    "t = 10\n",
    "C = B.filter(lambda x: x > t)\n",
    "print (C.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 5\n",
    "Modify the PMI example by sending a_dict and n_dict inside the closure. Do not use broadcast variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "\n",
    "lines = sc.textFile('data/adj_noun_pairs.txt')\n",
    "\n",
    "# Converting lines into word pairs. \n",
    "# Data is dirty: some lines have more than 2 words, so filter them out.\n",
    "pairs = lines.map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n",
    "pairs.cache()\n",
    "N = pairs.count()\n",
    "\n",
    "# Compute the frequency of each pair.\n",
    "# Ignore pairs that not frequent enough\n",
    "pair_freqs = pairs.map(lambda p: (p,1)) \\\n",
    "                  .reduceByKey(lambda f1, f2: f1 + f2) \\\n",
    "                  .filter(lambda pf: pf[1] >= 100)\n",
    "\n",
    "# Computing the frequencies of the adjectives and the nouns\n",
    "a_freqs = pairs.map(lambda p: (p[0],1)).reduceByKey(lambda x,y: x+y)\n",
    "n_freqs = pairs.map(lambda p: (p[1],1)).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "# Make a_dict and n_dict\n",
    "a_dict = a_freqs.collectAsMap()\n",
    "n_dict = n_freqs.collectAsMap()\n",
    "\n",
    "# Computing the PMI for a pair.\n",
    "def pmi_score(pair_freq, a_dict, n_dict):\n",
    "    w1, w2 = pair_freq[0]\n",
    "    f = pair_freq[1]\n",
    "    pmi = log(float(f)*N / (a_dict[w1]*n_dict[w2]), 2)\n",
    "    return pmi, (w1, w2)\n",
    "\n",
    "# Note:\n",
    "# Before broadcasting, a_dict and n_dict are <class 'dict'>\n",
    "# After broadcasting, a_dict and n_dict are <class 'pyspark.broadcast.Broadcast'>\n",
    "\n",
    "# Don't using broadcast variables way\n",
    "# Computing the PMI for all pairs. Using lamdba to pass a_dict and n_dict into function pmi_score.\n",
    "scored_pairs = pair_freqs.map(lambda x: pmi_score(x, a_dict, n_dict))\n",
    "\n",
    "# Show the top 10 samples\n",
    "scored_pairs.top(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-7ad6940ebebe>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-7ad6940ebebe>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    The following code creates an RDD with 4 partitions: partition 0, 1, 2, and 3.\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Question 6\n",
    "\n",
    "The following code creates an RDD with 4 partitions: partition 0, 1, 2, and 3.\n",
    "\n",
    "    A = sc.parallelize(xrange(100), 4)\n",
    "For each item in the RDD, add its partition number to it, and write the results to another RDD, i.e., the resulting RDD should contain:\n",
    "```\n",
    "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sc.parallelize(range(100), 4)\n",
    "def add_index(index, part):\n",
    "    for i in part:\n",
    "        i += index\n",
    "        yield i\n",
    "    \n",
    "B = A.mapPartitionsWithIndex(add_index)\n",
    "print (B.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quizz 2 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/sales.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all distinct countries.\n",
    "df.select('Country').distinct().show()\n",
    "\n",
    "# Find the Name and Price of sales records in Brazil.\n",
    "df.filter(\"Country = 'Brazil'\").select('Name', 'Price').show()\n",
    "\n",
    "# For each country, find the total Price.\n",
    "df.groupBy('Country').sum('Price') \\\n",
    "  .withColumnRenamed('sum(Price)', 'TotalPrice') \\\n",
    "  .show()\n",
    "\n",
    "# List countries by their total Price in descending order.\n",
    "df.groupBy('Country').sum('Price') \\\n",
    "  .withColumnRenamed('sum(Price)', 'TotalPrice') \\\n",
    "  .orderBy('TotalPrice', ascending = False) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv('data/countries.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each country, find the total Price. Replace the country names by their IDs using df2.\n",
    "df.groupBy('Country').sum('Price') \\\n",
    "  .join(df2, 'Country') \\\n",
    "  .withColumnRenamed('sum(Price)', 'TotalPrice') \\\n",
    "  .select('ID', 'TotalPrice') \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 6\n",
    "Rewrite the PageRank example using DataFrame API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "lines = spark.read.text(\"data/pagerank_data.txt\")\n",
    "# You can also test your program on the follow larger data set:\n",
    "# lines = spark.read.text(\"dblp.in\")\n",
    "\n",
    "a = lines.select(split(lines[0],' '))\n",
    "\n",
    "links = a.select(a[0][0].alias('src'), a[0][1].alias('dst'))\n",
    "links.show()\n",
    "outdegrees = links.groupBy('src').count()\n",
    "\n",
    "outdegrees = outdegrees.select('src', 'count')\n",
    "\n",
    "ranks = outdegrees.select('src', lit(1).alias('rank')) # lit(1) is the meaning of initializing to 1\n",
    "\n",
    "# number of objects --> num = 4\n",
    "num = ranks.count()\n",
    "\n",
    "for iteration in range(numOfIterations):\n",
    "    contribs = links.join(outdegrees, 'src').join(ranks, 'src') \\\n",
    "                    .select('*', (ranks['rank'] / outdegrees['count']).alias('contribs')) \\\n",
    "                    .withColumnRenamed('dst','dst1').groupBy('dst1').sum('contribs')\n",
    "    \n",
    "    ranks = contribs.select ('dst1', (contribs['sum(contribs)'] * 0.85 + 0.15 / num).alias('rank')) \\\n",
    "                    .withColumnRenamed('dst1', 'src')\n",
    "\n",
    "\n",
    "ranks.orderBy(desc('rank')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quizz 3 Algorithm Design"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 1\n",
    "Load it into spark and use divide-and-conquer to find the first (adj, noun) pair in which the noun is `'unification'`. Print the corresponding adjective. One solution is to use `filter()` to find all pairs where the noun is 'unification', and then report the first one. This is inefficient. The better idea is to find, in parallel, the first such pair in each partition (if one exists), and then find the first partition that returns such a pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPartitions = 10\n",
    "lines = sc.textFile(\"data/adj_noun_pairs.txt\", numPartitions)\n",
    "pairs = lines.map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n",
    "pairs.cache()\n",
    "\n",
    "def find_word(iterator):\n",
    "    for w in iterator:\n",
    "        if w[1] == \"unification\":\n",
    "            yield w\n",
    "            break\n",
    "print(pairs.mapPartitions(find_word).take(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 2\n",
    "Design a parallel divide-and-conquer algorithm for the following problem: Given two strings of equal length, compare them lexicographically. Output '<', '=', or '>', depending on the comparison result. The skeleton code is provided below.  Your code should run on all partitions of the rdd in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'abcccbcbcacaccacaabb'\n",
    "y = 'abcccbcccacaccacaabb'\n",
    "\n",
    "numPartitions = 4\n",
    "rdd = sc.parallelize(zip(x,y), numPartitions).cache()\n",
    "def char_compare(iterator):\n",
    "    for c in iterator:\n",
    "        if (ord(c[0]) - ord(c[1])) < 0:\n",
    "            yield -1\n",
    "        elif (ord(c[0]) - ord(c[1])) > 0:\n",
    "            yield 1\n",
    "res = rdd.mapPartitions(char_compare)\n",
    "if res.isEmpty():\n",
    "    print(\"=\")\n",
    "elif res.take(1)[0] == 1:\n",
    "    print(\">\")\n",
    "else:\n",
    "    print(\"<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quizz 4 Graph and Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertics DataFrame\n",
    "v = spark.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 37),\n",
    "  (\"d\", \"David\", 29),\n",
    "  (\"e\", \"Esther\", 32),\n",
    "  (\"f\", \"Fanny\", 38),\n",
    "  (\"g\", \"Gabby\", 60)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Edges DataFrame\n",
    "e = spark.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "  (\"f\", \"c\", \"follow\"),\n",
    "  (\"e\", \"f\", \"follow\"),\n",
    "  (\"e\", \"d\", \"friend\"),\n",
    "  (\"d\", \"a\", \"friend\"),\n",
    "  (\"a\", \"e\", \"friend\"),\n",
    "  (\"g\", \"e\", \"follow\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# Create a GraphFrame\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "g.vertices.show()\n",
    "g.edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 1-4\n",
    "Find Alice's two-hop neighbors' names, regardless of the edge type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Alice's two-hop neighbors' names, regardless of the edge type.\n",
    "Alice_two_hop = g.find(\"(a)-[]->(b); (b)-[]->(c)\").filter(\"a.name = 'Alice'\")\n",
    "Alice_two_hop.select(\"c.name\").show()\n",
    "\n",
    "# Redo the previous question, but exclude Alice's two-hop neighbors who have an edge back to Alice.\n",
    "Alice_two_hop_back = g.find(\"(a)-[]->(b); (b)-[]->(c); (c)-[]->(a)\").filter(\"a.name = 'Alice'\")\n",
    "Alice_two_hop_back.select('c.name').show()\n",
    "\n",
    "# Find all people who follow Charlie.\n",
    "who_follow_Charie = g.find(\"(a)-[e]->(b)\") \\\n",
    "                     .filter(\"b.name = 'Charlie' AND e.relationship = 'follow'\")\n",
    "who_follow_Charie.select(\"a.name\").show()\n",
    "\n",
    "# Find all people who are being followed by at least 2 people.\n",
    "e2 = g.edges.filter(\"relationship = 'follow'\")\n",
    "g2 = GraphFrame(v, e2)\n",
    "g2.vertices.join(g2.inDegrees, 'id', 'left_outer') \\\n",
    "           .where(\"inDegree >= 2\") \\\n",
    "           .select(\"name\") \\\n",
    "           .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 5\n",
    "\n",
    "Create a queue of 10 RDDs using this data set and feed it into a Spark Streaming program.  Your Spark Streaming algorithm should maintain a state that keeps track of the longest noun seen so far associated with each distinct adjective. After each RDD, print any 5 adjectives and their associated longest nouns, as well as the longest noun associated with the adjective 'good'. Note that not every line in the data set contains exactly two words, so make sure to clean the data as they are fed into the streaming program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 5)\n",
    "# Provide a checkpointing directory. Required for stateful transformations\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "numPartitions = 8\n",
    "rdd = sc.textFile('data/adj_noun_pairs.txt', numPartitions)\n",
    "rddQueue = rdd.randomSplit([1]*10, 123) # Split the RDD into 10 RDDs\n",
    "lines = ssc.queueStream(rddQueue)\n",
    "\n",
    "def updateFunc(newNoun, runningNoun):\n",
    "    if runningNoun is None:\n",
    "        return newNoun\n",
    "    elif len(newNoun) > len(runningNoun):\n",
    "        return newNoun\n",
    "    else:\n",
    "        return runningNoun\n",
    "\n",
    "pairs = lines.map(lambda l: tuple(l.split(\" \"))) \\\n",
    "             .filter(lambda p: len(p)==2) \\\n",
    "             .reduceByKey(lambda a, b: a if len(a) > len(b) else b) \\\n",
    "             .updateStateByKey(updateFunc)\n",
    "\n",
    "def printResults(rdd):\n",
    "    print (rdd.take(5))\n",
    "    print ('Longest noun associated with good:', rdd.lookup('good')[0])\n",
    "\n",
    "pairs.foreachRDD(printResults)\n",
    "\n",
    "# Strart spark stream\n",
    "ssc.start()\n",
    "print(\"Start\")\n",
    "ssc.awaitTermination(150)\n",
    "ssc.stop(False)\n",
    "print(\"Finished\")"
   ]
  }
 ]
}