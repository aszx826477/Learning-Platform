{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-UCGP8OJ:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-UCGP8OJ:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2bee5d08940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(age=11, name='Alice')\n",
      "Alice 11\n",
      "Alice 11\n",
      "<built-in method count of Row object at 0x000002BEE2B54410>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row(name=\"Alice\", age=11)\n",
    "print (row)\n",
    "print (row['name'], row['age'])\n",
    "print (row.name, row.age)\n",
    "\n",
    "row = Row(name=\"Alice\", age=11, count=1)\n",
    "print (row.count)\n",
    "print (row['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file at https://www.cse.ust.hk/msbd5003/data/building.csv\n",
    "\n",
    "df = spark.read.csv('data/building.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "|         1|         M1|         25|     AC1000|         USA|\n",
      "|         2|         M2|         27|     FN39TG|      France|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|\n",
      "|         4|         M4|         17|     GG1919|     Finland|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|\n",
      "|        10|        M10|         23|    ACMAX22|       China|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|\n",
      "|        14|        M14|         17|     GG1919|     Germany|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|\n",
      "|        19|        M19|         14|     GG1919|      Canada|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the content of the dataframe\n",
    "# Only show first twenty rows\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BuildingID: integer (nullable = true)\n",
      " |-- BuildingMgr: string (nullable = true)\n",
      " |-- BuildingAge: integer (nullable = true)\n",
      " |-- HVACproduct: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the dataframe schema in a tree format\n",
    "# Spark will automatically recognize the data format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(BuildingID=1, BuildingMgr='M1', BuildingAge=25, HVACproduct='AC1000', Country='USA'),\n",
       " Row(BuildingID=2, BuildingMgr='M2', BuildingAge=27, HVACproduct='FN39TG', Country='France'),\n",
       " Row(BuildingID=3, BuildingMgr='M3', BuildingAge=28, HVACproduct='JDNS77', Country='Brazil'),\n",
       " Row(BuildingID=4, BuildingMgr='M4', BuildingAge=17, HVACproduct='GG1919', Country='Finland')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD from the dataframe\n",
    "dfrdd = df.rdd\n",
    "dfrdd.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|BuildingID|     Country|\n",
      "+----------+------------+\n",
      "|         1|         USA|\n",
      "|         2|      France|\n",
      "|         3|      Brazil|\n",
      "|         4|     Finland|\n",
      "|         5|   Hong Kong|\n",
      "|         6|   Singapore|\n",
      "|         7|South Africa|\n",
      "|         8|   Australia|\n",
      "|         9|      Mexico|\n",
      "|        10|       China|\n",
      "|        11|     Belgium|\n",
      "|        12|     Finland|\n",
      "|        13|Saudi Arabia|\n",
      "|        14|     Germany|\n",
      "|        15|      Israel|\n",
      "|        16|      Turkey|\n",
      "|        17|       Egypt|\n",
      "|        18|   Indonesia|\n",
      "|        19|      Canada|\n",
      "|        20|   Argentina|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve specific columns from the dataframe\n",
    "df.select('BuildingID', 'Country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|BuildingID|123_test|\n",
      "+----------+--------+\n",
      "|         1|123_test|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selection and projection\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.where(\"Country='USA'\").select('BuildingID', lit('123_test')).show()\n",
    "# lit('OK') add a new column with default value 'OK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|HVACProduct|count|\n",
      "+-----------+-----+\n",
      "|    ACMAX22|    4|\n",
      "|     AC1000|    4|\n",
      "|     JDNS77|    4|\n",
      "|     FN39TG|    4|\n",
      "|     GG1919|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use GroupBy clause with dataframe \n",
    "df.groupBy('HVACProduct').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting SQL with DataFrame API\n",
    "\n",
    "The data files have been put to a public blob container, which can be accessed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "# Data files at https://www.cse.ust.hk/msbd5003/data\n",
    "\n",
    "dfCustomer = spark.read.csv('data/Customer.csv', header=True, inferSchema=True)\n",
    "dfProduct = spark.read.csv('data/Product.csv', header=True, inferSchema=True)\n",
    "dfDetail = spark.read.csv('data/SalesOrderDetail.csv', header=True, inferSchema=True)\n",
    "dfHeader = spark.read.csv('data/SalesOrderHeader.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+---------+\n",
      "|ProductID|Name                         |ListPrice|\n",
      "+---------+-----------------------------+---------+\n",
      "|680      |HL Road Frame - Black, 58    |1431.5   |\n",
      "|708      |Sport-100 Helmet, Black      |34.99    |\n",
      "|722      |LL Road Frame - Black, 58    |337.22   |\n",
      "|723      |LL Road Frame - Black, 60    |337.22   |\n",
      "|724      |LL Road Frame - Black, 62    |337.22   |\n",
      "|736      |LL Road Frame - Black, 44    |337.22   |\n",
      "|737      |LL Road Frame - Black, 48    |337.22   |\n",
      "|738      |LL Road Frame - Black, 52    |337.22   |\n",
      "|743      |HL Mountain Frame - Black, 42|1349.6   |\n",
      "|744      |HL Mountain Frame - Black, 44|1349.6   |\n",
      "|745      |HL Mountain Frame - Black, 48|1349.6   |\n",
      "|746      |HL Mountain Frame - Black, 46|1349.6   |\n",
      "|747      |HL Mountain Frame - Black, 38|1349.6   |\n",
      "|765      |Road-650 Black, 58           |782.99   |\n",
      "|766      |Road-650 Black, 60           |782.99   |\n",
      "|767      |Road-650 Black, 62           |782.99   |\n",
      "|768      |Road-650 Black, 44           |782.99   |\n",
      "|769      |Road-650 Black, 48           |782.99   |\n",
      "|770      |Road-650 Black, 52           |782.99   |\n",
      "|775      |Mountain-100 Black, 38       |3374.99  |\n",
      "+---------+-----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black'\n",
    "\n",
    "# Conpare a column with a constant/string\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+------------+\n",
      "|ProductID|Name                         |Double price|\n",
      "+---------+-----------------------------+------------+\n",
      "|680      |HL Road Frame - Black, 58    |2863.0      |\n",
      "|708      |Sport-100 Helmet, Black      |69.98       |\n",
      "|722      |LL Road Frame - Black, 58    |674.44      |\n",
      "|723      |LL Road Frame - Black, 60    |674.44      |\n",
      "|724      |LL Road Frame - Black, 62    |674.44      |\n",
      "|736      |LL Road Frame - Black, 44    |674.44      |\n",
      "|737      |LL Road Frame - Black, 48    |674.44      |\n",
      "|738      |LL Road Frame - Black, 52    |674.44      |\n",
      "|743      |HL Mountain Frame - Black, 42|2699.2      |\n",
      "|744      |HL Mountain Frame - Black, 44|2699.2      |\n",
      "|745      |HL Mountain Frame - Black, 48|2699.2      |\n",
      "|746      |HL Mountain Frame - Black, 46|2699.2      |\n",
      "|747      |HL Mountain Frame - Black, 38|2699.2      |\n",
      "|765      |Road-650 Black, 58           |1565.98     |\n",
      "|766      |Road-650 Black, 60           |1565.98     |\n",
      "|767      |Road-650 Black, 62           |1565.98     |\n",
      "|768      |Road-650 Black, 44           |1565.98     |\n",
      "|769      |Road-650 Black, 48           |1565.98     |\n",
      "|770      |Road-650 Black, 52           |1565.98     |\n",
      "|775      |Mountain-100 Black, 38       |6749.98     |\n",
      "+---------+-----------------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfProduct.where(dfProduct.Color=='Black') \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], (dfProduct.ListPrice * 2).alias('Double price')) \\\n",
    "         .show(truncate=False)\n",
    "\n",
    "# truncate --- 截短、缩短的意思\n",
    "# if truncate = True then the Name will be partially showed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+---------------+\n",
      "|ProductID|Name                     |(ListPrice * 2)|\n",
      "+---------+-------------------------+---------------+\n",
      "|680      |HL Road Frame - Black, 58|2863.0         |\n",
      "|706      |HL Road Frame - Red, 58  |2863.0         |\n",
      "|717      |HL Road Frame - Red, 62  |2863.0         |\n",
      "|718      |HL Road Frame - Red, 44  |2863.0         |\n",
      "|719      |HL Road Frame - Red, 48  |2863.0         |\n",
      "|720      |HL Road Frame - Red, 52  |2863.0         |\n",
      "|721      |HL Road Frame - Red, 56  |2863.0         |\n",
      "|722      |LL Road Frame - Black, 58|674.44         |\n",
      "|723      |LL Road Frame - Black, 60|674.44         |\n",
      "|724      |LL Road Frame - Black, 62|674.44         |\n",
      "|725      |LL Road Frame - Red, 44  |674.44         |\n",
      "|726      |LL Road Frame - Red, 48  |674.44         |\n",
      "|727      |LL Road Frame - Red, 52  |674.44         |\n",
      "|728      |LL Road Frame - Red, 58  |674.44         |\n",
      "|729      |LL Road Frame - Red, 60  |674.44         |\n",
      "|730      |LL Road Frame - Red, 62  |674.44         |\n",
      "|731      |ML Road Frame - Red, 44  |1189.66        |\n",
      "|732      |ML Road Frame - Red, 48  |1189.66        |\n",
      "|733      |ML Road Frame - Red, 52  |1189.66        |\n",
      "|734      |ML Road Frame - Red, 58  |1189.66        |\n",
      "+---------+-------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfProduct.where(dfProduct.ListPrice * 2 > 100) \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], dfProduct.ListPrice * 2) \\\n",
    "         .show(truncate=False)\n",
    "\n",
    "# where and filter have the same function\n",
    "# where is the another name of filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------+---------+\n",
      "|ProductID|Name                      |ListPrice|\n",
      "+---------+--------------------------+---------+\n",
      "|860      |Half-Finger Gloves, L     |24.49    |\n",
      "|859      |Half-Finger Gloves, M     |24.49    |\n",
      "|858      |Half-Finger Gloves, S     |24.49    |\n",
      "|708      |Sport-100 Helmet, Black   |34.99    |\n",
      "|862      |Full-Finger Gloves, M     |37.99    |\n",
      "|861      |Full-Finger Gloves, S     |37.99    |\n",
      "|863      |Full-Finger Gloves, L     |37.99    |\n",
      "|841      |Men's Sports Shorts, S    |59.99    |\n",
      "|849      |Men's Sports Shorts, M    |59.99    |\n",
      "|851      |Men's Sports Shorts, XL   |59.99    |\n",
      "|850      |Men's Sports Shorts, L    |59.99    |\n",
      "|815      |LL Mountain Front Wheel   |60.745   |\n",
      "|868      |Women's Mountain Shorts, M|69.99    |\n",
      "|869      |Women's Mountain Shorts, L|69.99    |\n",
      "|867      |Women's Mountain Shorts, S|69.99    |\n",
      "|853      |Women's Tights, M         |74.99    |\n",
      "|854      |Women's Tights, L         |74.99    |\n",
      "|852      |Women's Tights, S         |74.99    |\n",
      "|818      |LL Road Front Wheel       |85.565   |\n",
      "|823      |LL Mountain Rear Wheel    |87.745   |\n",
      "+---------+--------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black' \n",
    "# ORDER BY ProductID\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .orderBy('ListPrice')\\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113295|Sport-100 Helmet,...|   20.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71797|            111045|LL Road Frame - B...|  202.332|       3|\n",
      "|       71783|            110730|LL Road Frame - B...|  202.332|       6|\n",
      "|       71938|            113297|LL Road Frame - B...|  202.332|       3|\n",
      "|       71915|            113090|LL Road Frame - B...|  202.332|       2|\n",
      "|       71815|            111451|LL Road Frame - B...|  202.332|       1|\n",
      "|       71797|            111044|LL Road Frame - B...|  202.332|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71936|            113260|HL Mountain Frame...|   809.76|       4|\n",
      "|       71899|            112937|HL Mountain Frame...|   809.76|       1|\n",
      "|       71845|            112137|HL Mountain Frame...|   809.76|       2|\n",
      "|       71832|            111806|HL Mountain Frame...|   809.76|       4|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71936|            113235|HL Mountain Frame...|   809.76|       4|\n",
      "|       71845|            112134|HL Mountain Frame...|   809.76|       3|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders and details on black product,\n",
    "# return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesOrderDetail.ProductID = Product.ProductID AND Color = 'Black'\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "# Spark SQL supports natural joins\n",
    "\n",
    "dfDetail.join(dfProduct, 'ProductID') \\\n",
    "        .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') \\\n",
    "        .filter(\"Color='Black'\")\\\n",
    "        .show()\n",
    "\n",
    "# If we move the filter to after select, it still works.  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113278|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71936|            113228|Sport-100 Helmet,...|   20.994|       1|\n",
      "|       71902|            112980|Sport-100 Helmet,...|   20.994|       2|\n",
      "|       71797|            111075|Sport-100 Helmet,...|   20.994|       6|\n",
      "|       71784|            110794|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71783|            110751|Sport-100 Helmet,...|   20.994|      10|\n",
      "|       71782|            110709|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71938|            113295|Sport-100 Helmet,...|   20.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71938|            113282|Sport-100 Helmet,...|   20.994|       3|\n",
      "|       71902|            112995|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71863|            112395|Sport-100 Helmet,...|   20.994|       1|\n",
      "|       71797|            111038|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71784|            110753|Sport-100 Helmet,...|   20.994|       2|\n",
      "|       71783|            110749|Sport-100 Helmet,...|  19.2445|      15|\n",
      "|       71782|            110708|Sport-100 Helmet,...|   20.994|       6|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71938|            113295|Sport-100 Helmet,...|   20.994|       5|\n",
      "|       71902|            112988|Sport-100 Helmet,...|   20.994|       4|\n",
      "|       71797|            111082|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71784|            110795|Sport-100 Helmet,...|  20.2942|      12|\n",
      "|       71783|            110752|Sport-100 Helmet,...|  20.2942|      11|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71797|            111045|LL Road Frame - B...|  202.332|       3|\n",
      "|       71783|            110730|LL Road Frame - B...|  202.332|       6|\n",
      "|       71938|            113297|LL Road Frame - B...|  202.332|       3|\n",
      "|       71915|            113090|LL Road Frame - B...|  202.332|       2|\n",
      "|       71815|            111451|LL Road Frame - B...|  202.332|       1|\n",
      "|       71797|            111044|LL Road Frame - B...|  202.332|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71936|            113260|HL Mountain Frame...|   809.76|       4|\n",
      "|       71899|            112937|HL Mountain Frame...|   809.76|       1|\n",
      "|       71845|            112137|HL Mountain Frame...|   809.76|       2|\n",
      "|       71832|            111806|HL Mountain Frame...|   809.76|       4|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71936|            113235|HL Mountain Frame...|   809.76|       4|\n",
      "|       71845|            112134|HL Mountain Frame...|   809.76|       3|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [SalesOrderID#306, SalesOrderDetailID#307, Name#263, UnitPrice#310, OrderQty#308]\n",
      "+- *(2) BroadcastHashJoin [ProductID#309], [ProductID#262], Inner, BuildLeft\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[3, int, true] as bigint)))\n",
      "   :  +- *(1) Project [SalesOrderID#306, SalesOrderDetailID#307, OrderQty#308, ProductID#309, UnitPrice#310]\n",
      "   :     +- *(1) Filter isnotnull(ProductID#309)\n",
      "   :        +- *(1) FileScan csv [SalesOrderID#306,SalesOrderDetailID#307,OrderQty#308,ProductID#309,UnitPrice#310] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/yelbee/Desktop/BDT-Study-Projects/BigDataCom/SparkTest/data/SalesOrd..., PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double>\n",
      "   +- *(2) Project [ProductID#262, Name#263]\n",
      "      +- *(2) Filter ((isnotnull(Color#265) && (Color#265 = Black)) && isnotnull(ProductID#262))\n",
      "         +- *(2) FileScan csv [ProductID#262,Name#263,Color#265] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/yelbee/Desktop/BDT-Study-Projects/BigDataCom/SparkTest/data/Product...., PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string>\n"
     ]
    }
   ],
   "source": [
    "# This also works:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.show()\n",
    "d2 = d1.filter(\"Color = 'Black'\")\n",
    "d2.show() \n",
    "\n",
    "# Dataframe is on the top of RDD, also have re-compute character (Lineage Graph)\n",
    "# Dataframe and SQL share the same optimization/execution pipline\n",
    "\n",
    "d2.explain()\n",
    "\n",
    "# Use explain() to find the true sequence of running the code\n",
    "# It isn't the way we write the code\n",
    "# It do filter --> project --> join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-2d222b68-b19a-4db5-8f83-b91a82b73836/pyspark_runner.py\", line 189, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 5, in <module>\n",
       "  File \"/csproject/msbd5003/python/pyspark/sql/dataframe.py\", line 1078, in filter\n",
       "    jdf = self._jdf.filter(condition)\n",
       "  File \"/csproject/msbd5003/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
       "    answer, self.gateway_client, self.target_id, self.name)\n",
       "  File \"/csproject/msbd5003/python/pyspark/sql/utils.py\", line 69, in deco\n",
       "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
       "AnalysisException: u\"cannot resolve '`Color`' given input columns: [OrderQty, UnitPrice, SalesOrderID, Name, SalesOrderDetailID]; line 1 pos 0;\\n'Filter ('Color = Black)\\n+- Relation[SalesOrderID#435,SalesOrderDetailID#436,Name#437,UnitPrice#438,OrderQty#439] csv\\n\"\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will report an error:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.write.csv('temp.csv', mode = 'overwrite', header = True)\n",
    "d2 = spark.read.csv('temp.csv', header = True, inferSchema = True)\n",
    "d2.filter(\"Color = 'Black'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|SalesOrderID|\n",
      "+------------+\n",
      "|       71902|\n",
      "|       71832|\n",
      "|       71915|\n",
      "|       71831|\n",
      "|       71898|\n",
      "|       71935|\n",
      "|       71938|\n",
      "|       71845|\n",
      "|       71783|\n",
      "|       71815|\n",
      "|       71936|\n",
      "|       71863|\n",
      "|       71780|\n",
      "|       71782|\n",
      "|       71899|\n",
      "|       71784|\n",
      "|       71797|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders that include at least one black product, \n",
    "# return the product SalesOrderID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT DISTINCT SalesOrderID\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "dfDetail.join(dfProduct.filter(\"Color='Black'\"), 'ProductID') \\\n",
    "        .select('SalesOrderID') \\\n",
    "        .distinct() \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many colors in the products?\n",
    "\n",
    "# SELECT COUNT(DISTINCT Color)\n",
    "# FROM SalesLT.Product\n",
    "\n",
    "dfProduct.select('Color').distinct().count()\n",
    "\n",
    "# It's 1 more than standard SQL.  In standard SQL, COUNT() does not count NULLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71867|             858.9|\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71915|1732.8899999999999|\n",
      "|       71946|            31.584|\n",
      "|       71895|221.25600000000003|\n",
      "|       71816|2847.4079999999994|\n",
      "|       71831|          1712.946|\n",
      "|       71923|         96.108824|\n",
      "|       71858|11528.844000000001|\n",
      "|       71917|            37.758|\n",
      "|       71897|          10585.05|\n",
      "|       71885|           524.664|\n",
      "|       71856|500.30400000000003|\n",
      "|       71898| 53248.69200000002|\n",
      "|       71774|           713.796|\n",
      "|       71796| 47848.02600000001|\n",
      "|       71935|5533.8689079999995|\n",
      "|       71938|         74160.228|\n",
      "|       71845|        34118.5356|\n",
      "+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order, \n",
    "# return SalesOrderID and total price (column name should be ‘totalprice’)\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail.OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71858|11528.844000000001|\n",
      "|       71897|          10585.05|\n",
      "|       71898| 53248.69200000002|\n",
      "|       71796| 47848.02600000001|\n",
      "|       71938|         74160.228|\n",
      "|       71845|        34118.5356|\n",
      "|       71783|      65683.367986|\n",
      "|       71936| 79589.61602399996|\n",
      "|       71780|29923.007999999998|\n",
      "|       71782| 33319.98600000001|\n",
      "|       71784| 89869.27631400003|\n",
      "|       71797| 65123.46341800001|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|         26677.884|\n",
      "|       71832|      16883.748108|\n",
      "|       71938|         33779.448|\n",
      "|       71845|         18109.836|\n",
      "|       71783|15524.117476000003|\n",
      "|       71936| 44490.29042399999|\n",
      "|       71780|16964.321999999996|\n",
      "|       71797|      27581.613792|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price on the black products of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesLT.SalesOrderDetail.ProductID = SalesLT.Product.ProductID AND Color = 'Black'\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .join(dfProduct.where(\"Color = 'Black'\"), 'ProductID') \\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+-------------+\n",
      "|CustomerID|   FirstName|    LastName|sum(OrderQty)|\n",
      "+----------+------------+------------+-------------+\n",
      "|     30050|     Krishna|Sunkammurali|           89|\n",
      "|     29796|         Jon|      Grande|           65|\n",
      "|     29957|       Kevin|         Liu|           62|\n",
      "|     29929|     Jeffrey|       Kurtz|           46|\n",
      "|     29546| Christopher|        Beck|           45|\n",
      "|     29922|      Pamala|        Kotc|           34|\n",
      "|     30113|        Raja|   Venugopal|           34|\n",
      "|     29938|       Frank|    Campbell|           29|\n",
      "|     29736|       Terry|   Eminhizer|           23|\n",
      "|     29485|   Catherine|        Abel|           10|\n",
      "|     30019|     Matthew|      Miller|            9|\n",
      "|     29932|     Rebecca|      Laszlo|            7|\n",
      "|     29975|      Walter|        Mays|            5|\n",
      "|     29638|    Rosmarie|     Carroll|            2|\n",
      "|     30089|Michael John|      Troyer|            1|\n",
      "|     29568|      Donald|     Blanton|            1|\n",
      "|     29531|        Cory|       Booth|            1|\n",
      "+----------+------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each customer, find the total quantity of black products bought.\n",
    "# Report CustomerID, FirstName, LastName, and total quantity\n",
    "\n",
    "# select saleslt.customer.customerid, FirstName, LastName, sum(orderqty)\n",
    "# from saleslt.customer\n",
    "# left outer join \n",
    "# (\n",
    "# saleslt.salesorderheader\n",
    "# join saleslt.salesorderdetail\n",
    "# on saleslt.salesorderdetail.salesorderid = saleslt.salesorderheader.salesorderid\n",
    "# join saleslt.product\n",
    "# on saleslt.product.productid = saleslt.salesorderdetail.productid and color = 'black'\n",
    "# )\n",
    "# on saleslt.customer.customerid = saleslt.salesorderheader.customerid\n",
    "# group by saleslt.customer.customerid, FirstName, LastName\n",
    "# order by sum(orderqty) desc\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID')\\     # inner join\n",
    "             .where('Color = \"Black\"') \\\n",
    "             .join(dfHeader, 'SalesOrderID')\\   # inner join\n",
    "             .groupBy('CustomerID').sum('OrderQty')\n",
    "dfCustomer.join(d1, 'CustomerID', 'left_outer')\\ # left_outer join\n",
    "          .select('CustomerID', 'FirstName', 'LastName', 'sum(OrderQty)')\\\n",
    "          .orderBy('sum(OrderQty)', ascending=False)\\\n",
    "          .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Embed SQL queries\n",
    "\n",
    "You can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the dataframe as a temporary view called HVAC\n",
    "df.createOrReplaceTempView('HVAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "|         1|         M1|         25|     AC1000|         USA|\n",
      "|         2|         M2|         27|     FN39TG|      France|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|\n",
      "|         4|         M4|         17|     GG1919|     Finland|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|\n",
      "|        10|        M10|         23|    ACMAX22|       China|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|\n",
      "|        14|        M14|         17|     GG1919|     Germany|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|\n",
      "|        19|        M19|         14|     GG1919|      Canada|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|HVACproduct|count(1)|\n",
      "+-----------+--------+\n",
      "|    ACMAX22|       3|\n",
      "|     AC1000|       3|\n",
      "|     JDNS77|       4|\n",
      "|     FN39TG|       4|\n",
      "|     GG1919|       4|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can even mix DataFrame API with SQL:\n",
    "df.where('BuildingAge >= 10').createOrReplaceTempView('OldBuildings')\n",
    "spark.sql('SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|HVACproduct|count|\n",
      "+-----------+-----+\n",
      "|    ACMAX22|    3|\n",
      "|     AC1000|    3|\n",
      "|     JDNS77|    4|\n",
      "|     FN39TG|    4|\n",
      "|     GG1919|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1 = spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10')\n",
    "d1.groupBy('HVACproduct').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|slen|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|         1|         M1|         25|     AC1000|         USA|   5|\n",
      "|         2|         M2|         27|     FN39TG|      France|   8|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|   8|\n",
      "|         4|         M4|         17|     GG1919|     Finland|   9|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|  11|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|  11|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|  14|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|  11|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|   8|\n",
      "|        10|        M10|         23|    ACMAX22|       China|   7|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|   9|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|   9|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|  14|\n",
      "|        14|        M14|         17|     GG1919|     Germany|   9|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|   8|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|   8|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|   7|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|  11|\n",
      "|        19|        M19|         14|     GG1919|      Canada|   8|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|  11|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF\n",
    "# UDF（User Defined Function）：spark SQL中用户自定义函数，用法和spark SQL中的内置函数类似；\n",
    "# 是saprk SQL中内置函数无法满足要求，用户根据业务需求自定义的函数\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "slen = udf(lambda s: len(s)+2, IntegerType())\n",
    "df.select('*', slen(df['Country']).alias('slen')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|slen|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|         1|         M1|         25|     AC1000|         USA|   3|\n",
      "|         2|         M2|         27|     FN39TG|      France|   6|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|   6|\n",
      "|         4|         M4|         17|     GG1919|     Finland|   7|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|   9|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|   9|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|  12|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|   9|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|   6|\n",
      "|        10|        M10|         23|    ACMAX22|       China|   5|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|   7|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|   7|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|  12|\n",
      "|        14|        M14|         17|     GG1919|     Germany|   7|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|   6|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|   6|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|   5|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|   9|\n",
      "|        19|        M19|         14|     GG1919|      Canada|   6|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|   9|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('slen', lambda s: len(s), IntegerType())\n",
    "spark.sql('SELECT *, slen(Country) AS slen FROM HVAC').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Flexible Data Model\n",
    "\n",
    "Sample data file at\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/products.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dimensions: struct (nullable = true)\n",
      " |    |-- height: double (nullable = true)\n",
      " |    |-- length: double (nullable = true)\n",
      " |    |-- width: double (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- warehouseLocation: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('data/products.json')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+----------------+-----+-----------+-----------------+\n",
      "|      dimensions| id|            name|price|       tags|warehouseLocation|\n",
      "+----------------+---+----------------+-----+-----------+-----------------+\n",
      "|[9.5, 7.0, 12.0]|  2|An ice sculpture| 12.5|[cold, ice]|   [-78.75, 20.4]|\n",
      "| [1.0, 3.1, 1.0]|  3|    A blue mouse| 25.5|       null|    [54.4, -32.7]|\n",
      "+----------------+---+----------------+-----+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() # 只会显示第一层级的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing nested fields\n",
    "\n",
    "df.select(df['dimensions.height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height')\\\n",
    "  .filter(\"tags[0] = 'cold' AND warehouseLocation.latitude < 0\")\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dimensions=Row(height=9.5, length=7.0, width=12.0), id=2, name='An ice sculpture', price=12.5, tags=['cold', 'ice'], warehouseLocation=Row(latitude=-78.75, longitude=20.4)),\n",
       " Row(dimensions=Row(height=1.0, length=3.1, width=1.0), id=3, name='A blue mouse', price=25.5, tags=None, warehouseLocation=Row(latitude=54.4, longitude=-32.7))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Converting between RDD and DataFrame\n",
    "\n",
    "Sample data file at:\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/people.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Michael', 29), ('Andy', 30), ('Justin', 19)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"data/people.txt\")\n",
    "\n",
    "def parse(l):\n",
    "    a = l.split(',')\n",
    "    return (a[0], int(a[1]))\n",
    "\n",
    "rdd = lines.map(parse)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|     _1| _2|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of tuples, schema is inferred\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of tuples with column names, type is inferred\n",
    "df = spark.createDataFrame(rdd, ['name', 'age'])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 29|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of Rows, type is given in the Row objects\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rdd_rows = rdd.map(lambda p: Row(name = p[0], age = p[1]))\n",
    "df = spark.createDataFrame(rdd_rows)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age| name|\n",
      "+----+-----+\n",
      "|  11|Alice|\n",
      "|null|  Bob|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Row fields with types incompatible with that of previous rows will be turned into nulls\n",
    "row1 = Row(name=\"Alice\", age=11) # 注意有这个小细节\n",
    "row2 = Row(name=\"Bob\", age='12')\n",
    "rdd_rows = sc.parallelize([row1, row2])\n",
    "df1 = spark.createDataFrame(rdd_rows)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name: Justin']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd returns the content as an RDD of Rows\n",
    "teenagers = df.filter('age >= 13 and age <= 19')\n",
    "\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name)\n",
    "teenNames.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "DataFrames are stored using columnar storage with compression\n",
    "\n",
    "RDDs are stored using row storage without compression\n",
    "\n",
    "The RDD view of DataFrame just provides an interface, the Row objects are constructed on the fly and do not necessarily represent the internal storage format of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "|  5|  5|\n",
      "|  6|  6|\n",
      "|  7|  7|\n",
      "|  8|  8|\n",
      "|  9|  9|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = range(10)\n",
    "df = spark.createDataFrame(zip(data, data))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The 'closure' behaviour in RDD doesn't seem to exist for DataFrames\n",
    "\n",
    "x = 5\n",
    "df1 = df.filter(df._1 < x)\n",
    "df1.show()\n",
    "x = 3\n",
    "df1.show() # 并没有重计算，这与之前的RDD不一样，前面有一部分的理解需要矫正，重新研究和讨论一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(_1#251L) && (_1#251L < 5))\n",
      "+- Scan ExistingRDD[_1#251L,_2#252L]\n"
     ]
    }
   ],
   "source": [
    "# Because of the Catalyst optimizer !\n",
    "\n",
    "df1.explain() # x的值虽然改变了，但是由于catalyst优化器的缘故，优化成了一个常数5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [(((cast((_1#251L * 2) as double) + 2.5) + 1.0) + 1.0) AS ((((_1 * 2) + 2.5) + 1) + 1)#276]\n",
      "+- Scan ExistingRDD[_1#251L,_2#252L]\n",
      "+----------------------------+\n",
      "|((((_1 * 2) + 2.5) + 1) + 1)|\n",
      "+----------------------------+\n",
      "|                         4.5|\n",
      "|                         6.5|\n",
      "|                         8.5|\n",
      "|                        10.5|\n",
      "|                        12.5|\n",
      "|                        14.5|\n",
      "|                        16.5|\n",
      "|                        18.5|\n",
      "|                        20.5|\n",
      "|                        22.5|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f():\n",
    "    return x/2\n",
    "x = 5\n",
    "df1 = df.select(df._1 * 2 + f() + 1 + 1)\n",
    "df1.explain()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "x = 5\n",
    "a = rdd.filter(lambda z: z < x)\n",
    "print a.take(10)\n",
    "x = 3\n",
    "print a.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += 1\n",
    "\n",
    "df.foreach(increment_counter)\n",
    "\n",
    "print counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
