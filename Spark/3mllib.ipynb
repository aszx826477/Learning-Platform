{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# 把调用的package信息写的更详细，避免只用*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.122.153:4049\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f15e6f1eb00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic example on Transformer and Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data from a list of (label, features) tuples.\n",
    "# Dense Vectors are just NumPy arrays\n",
    "\n",
    "training = spark.createDataFrame([\n",
    "    (1, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01) # 训练10次，正则化系数0.01\n",
    "\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "print (lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid = LogisticRegression_c4e7c61455a1, numClasses = 2, numFeatures = 3\n",
      "Model 1's trained coefficients:  [-3.1009356010205327,2.60821473832145,-0.38017912254303127]\n"
     ]
    }
   ],
   "source": [
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model1 = lr.fit(training) # lr是一个estimator，它会返回一个Model\n",
    "\n",
    "print (model1)\n",
    "# model1 is a Model (i.e., a transformer produced by an Estimator)\n",
    "print (\"Model 1's trained coefficients: \", model1.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2's trained coefficients:  [-1.4313658815706816,0.4320887101487753,-0.1492041947797503]\n"
     ]
    }
   ],
   "source": [
    "# We may alternatively specify parameters using a Python dictionary as a paramMap\n",
    "\n",
    "# 使用python的字典\n",
    "paramMap = {lr.maxIter: 20}\n",
    "paramMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\n",
    "\n",
    "# 训练30次，正则化系数0.1 - 与model1不一样的地方\n",
    "\n",
    "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # Specify multiple Params.\n",
    "\n",
    "# threshold -- 阈值的意思，但是具体代表什么，有什么作用？待思考\n",
    "\n",
    "# You can combine paramMaps, which are python dictionaries.\n",
    "paramMap[lr.probabilityCol] = \"myProbability\"  # Change output column name\n",
    "\n",
    "# Now learn a new model using the paramMapCombined parameters.\n",
    "# paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "model2 = lr.fit(training, paramMap)\n",
    "print (\"Model 2's trained coefficients: \", model2.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------------+--------------------+----------+\n",
      "| id|      features|       rawPrediction|         probability|prediction|\n",
      "+---+--------------+--------------------+--------------------+----------+\n",
      "|  1|[-1.0,1.5,1.3]|[-6.5872014439355...|[0.00137599470692...|       1.0|\n",
      "|  2|[3.0,2.0,-0.1]|[3.98018281942566...|[0.98166040093741...|       0.0|\n",
      "|  3|[0.0,2.2,-1.5]|[-6.3765177028604...|[0.00169814755783...|       1.0|\n",
      "+---+--------------+--------------------+--------------------+----------+\n",
      "\n",
      "+---+--------------+--------------------+--------------------+----------+\n",
      "| id|      features|       rawPrediction|       myProbability|prediction|\n",
      "+---+--------------+--------------------+--------------------+----------+\n",
      "|  1|[-1.0,1.5,1.3]|[-2.8046569418746...|[0.05707304171033...|       1.0|\n",
      "|  2|[3.0,2.0,-0.1]|[2.49587635664203...|[0.92385223117040...|       0.0|\n",
      "|  3|[0.0,2.2,-1.5]|[-2.0935249027914...|[0.10972776114779...|       1.0|\n",
      "+---+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "test = spark.createDataFrame([\n",
    "    (1, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (2, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (3, Vectors.dense([0.0, 2.2, -1.5]))], [\"id\", \"features\"])\n",
    "\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "# LogisticRegression.transform will only use the 'features' column.\n",
    "# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n",
    "# 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
    "\n",
    "model1.transform(test).show()\n",
    "model2.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"A b c d spark spark\", 1),\n",
    "    (1, \"b d\", 0),\n",
    "    (2, \"SPARK f g h\", 1),\n",
    "    (3, \"hadoop mapreduce\", 0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+--------------------+\n",
      "| id|               text|label|               words|\n",
      "+---+-------------------+-----+--------------------+\n",
      "|  0|A b c d spark spark|    1|[a, b, c, d, spar...|\n",
      "|  1|                b d|    0|              [b, d]|\n",
      "|  2|        SPARK f g h|    1|    [spark, f, g, h]|\n",
      "|  3|   hadoop mapreduce|    0| [hadoop, mapreduce]|\n",
      "+---+-------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A tokenizer converts the input string to lowercase and then splits it by white spaces.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokenizer.transform(training).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+--------------------+\n",
      "| id|               text|label|               words|\n",
      "+---+-------------------+-----+--------------------+\n",
      "|  0|A b c d spark spark|    1|[A, b, c, d, spar...|\n",
      "|  1|                b d|    0|              [b, d]|\n",
      "|  2|        SPARK f g h|    1|    [SPARK, f, g, h]|\n",
      "|  3|   hadoop mapreduce|    0| [hadoop, mapreduce]|\n",
      "+---+-------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The same can be achieved by DataFrameAPI:\n",
    "# But you will need to wrap it as a transformer to use it in a pipeline.\n",
    "# 但是用这个方法不会转换大小写\n",
    "\n",
    "training.select('*', split(training['text'],' ').alias('words')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+--------------------------+----------------------------------------------------------------+\n",
      "|id |text               |label|words                     |features                                                        |\n",
      "+---+-------------------+-----+--------------------------+----------------------------------------------------------------+\n",
      "|0  |A b c d spark spark|1    |[a, b, c, d, spark, spark]|(262144,[27526,28698,30913,227410,234657],[1.0,1.0,1.0,1.0,2.0])|\n",
      "|1  |b d                |0    |[b, d]                    |(262144,[27526,30913],[1.0,1.0])                                |\n",
      "|2  |SPARK f g h        |1    |[spark, f, g, h]          |(262144,[15554,24152,51505,234657],[1.0,1.0,1.0,1.0])           |\n",
      "|3  |hadoop mapreduce   |0    |[hadoop, mapreduce]       |(262144,[42633,155117],[1.0,1.0])                               |\n",
      "+---+-------------------+-----+--------------------------+----------------------------------------------------------------+\n",
      "\n",
      "Row(features=SparseVector(262144, {27526: 1.0, 28698: 1.0, 30913: 1.0, 227410: 1.0, 234657: 2.0}))\n"
     ]
    }
   ],
   "source": [
    "# Maps a sequence of terms to their term frequencies(TF) using the hashing trick.\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "a = hashingTF.transform(tokenizer.transform(training))\n",
    "a.show(truncate=False)\n",
    "\n",
    "print (a.select('features').first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|              text|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|       spark i j k|    [spark, i, j, k]|(262144,[20197,24...|[-0.7500987629693...|[0.32079978124885...|       1.0|\n",
      "|  5|             l m n|           [l, m, n]|(262144,[18910,10...|[1.58033363406982...|[0.82925176350809...|       0.0|\n",
      "|  6|spark hadoop spark|[spark, hadoop, s...|(262144,[155117,2...|[-0.7053638244645...|[0.33062407361579...|       1.0|\n",
      "|  7|     apache hadoop|    [apache, hadoop]|(262144,[66695,15...|[3.95550096961367...|[0.98121072417264...|       0.0|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lr is an estimator\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "# Now we are ready to assumble the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "                            \n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "model.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## Example: Analyzing food inspection data using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data at https://www.cse.ust.hk/msbd5003/data/Food_Inspections1.csv\n",
    "\n",
    "inspections = spark.read.csv('data/Food_Inspections1.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at its schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: double (nullable = true)\n",
      " |-- _c15: double (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspections.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-------+--------------------+---------------+--------------------+-------+---+-----+----------+--------------------+------------------+--------------------+------------------+------------------+--------------------+\n",
      "|   _c0|                 _c1|                 _c2|    _c3|                 _c4|            _c5|                 _c6|    _c7|_c8|  _c9|      _c10|                _c11|              _c12|                _c13|              _c14|              _c15|                _c16|\n",
      "+------+--------------------+--------------------+-------+--------------------+---------------+--------------------+-------+---+-----+----------+--------------------+------------------+--------------------+------------------+------------------+--------------------+\n",
      "|413707|       LUNA PARK INC| LUNA PARK  DAY CARE|2049789|Children's Servic...|  Risk 1 (High)|  3250 W FOSTER AVE |CHICAGO| IL|60625|09/21/2010|  License-Task Force|              Fail|24. DISH WASHING ...| 41.97583445690982| -87.7107455232781|(41.9758344569098...|\n",
      "|391234|       CAFE SELMARIE|       CAFE SELMARIE|1069067|          Restaurant|  Risk 1 (High)| 4729 N LINCOLN AVE |CHICAGO| IL|60625|09/21/2010|             Canvass|              Fail|2. FACILITIES TO ...| 41.96740659751604|-87.68761642361608|(41.9674065975160...|\n",
      "|413751|          MANCHU WOK|MANCHU WOK (T3-H/...|1909522|          Restaurant|  Risk 1 (High)|  11601 W TOUHY AVE |CHICAGO| IL|60666|09/21/2010|             Canvass|              Pass|33. FOOD AND NON-...|42.008536400868735|-87.91442843927047|(42.0085364008687...|\n",
      "|413708|BENCHMARK HOSPITA...|BENCHMARK HOSPITA...|2049411|          Restaurant|  Risk 1 (High)|325 N LA SALLE ST...|CHICAGO| IL|60654|09/21/2010|Task Force Liquor...|              Pass|                null| 41.88819879207664|-87.63236298373182|(41.8881987920766...|\n",
      "|413722|           JJ BURGER|           JJ BURGER|2055016|          Restaurant|Risk 2 (Medium)|   749 S CICERO AVE |CHICAGO| IL|60644|09/21/2010|             License|              Pass|                null| 41.87082601444883|-87.74476763884662|(41.8708260144488...|\n",
      "|413752|GOLDEN HOOKS FISH...|GOLDEN HOOKS FISH...|2042435|          Restaurant|Risk 2 (Medium)|   3958 W MONROE ST |CHICAGO| IL|60624|09/21/2010|Short Form Complaint|              Pass|                null| 41.87987261425607|-87.72551692436804|(41.8798726142560...|\n",
      "|413714|THE DOCK AT MONTR...|THE DOCK AT MONTR...|2043260|          Restaurant|  Risk 1 (High)|  4400 N SIMONDS DR |CHICAGO| IL|60640|09/21/2010|             License|              Fail|                null| 41.96390893734172|-87.63863624840039|(41.9639089373417...|\n",
      "|413753|CLARK FOOD & CIGA...|                null|2042203|       Grocery Store|   Risk 3 (Low)|6761 N CLARK ST B...|CHICAGO| IL|60626|09/21/2010|             Canvass|              Pass|                null|  42.0053117273606|-87.67294053846207|(42.0053117273606...|\n",
      "|120580|          SUSHI PINK|          SUSHI PINK|1847340|          Restaurant|  Risk 1 (High)|909 W WASHINGTON ...|CHICAGO| IL|60607|09/21/2010|             Canvass|              Pass|32. FOOD AND NON-...|41.882987317760424|-87.65014022876997|(41.8829873177604...|\n",
      "|401216|       M.H.R.,L.L.C.|       M.H.R.,L.L.C.|1621323|          Restaurant|Risk 2 (Medium)|   623 S WABASH AVE |CHICAGO| IL|60605|09/21/2010|             Canvass|   Out of Business|                null| 41.87390845559158|-87.62583770570953|(41.8739084555915...|\n",
      "|413715|              NABO'S|              NABO'S|1931861|          Restaurant|  Risk 1 (High)|    3351 N BROADWAY |CHICAGO| IL|60657|09/21/2010|Canvass Re-Inspec...|              Pass|19. OUTSIDE GARBA...| 41.94334005547684|-87.64466387044959|(41.9433400554768...|\n",
      "|413721|THE NICHOLSON SCHOOL|THE NICHOLSON SCHOOL|2002702|Daycare (2 - 6 Ye...|  Risk 1 (High)| 1700 W CORTLAND ST |CHICAGO| IL|60622|09/21/2010|             License|              Pass|                null| 41.91618227133264| -87.6703413842735|(41.9161822713326...|\n",
      "|401215|       M.H.R.,L.L.C.|       M.H.R.,L.L.C.|1621322|          Restaurant|  Risk 1 (High)| 600 S MICHIGAN AVE |CHICAGO| IL|60605|09/21/2010|             Canvass|   Out of Business|                null| 41.87437161535891|-87.62437952778167|(41.8743716153589...|\n",
      "|420207|  WHOLE FOODS MARKET|  WHOLE FOODS MARKET|1933690|       Grocery Store|  Risk 1 (High)|1550 N KINGSBURY ST |CHICAGO| IL|60642|09/21/2010|           Complaint|              Pass|32. FOOD AND NON-...| 41.90939878780941|-87.65305069789407|(41.9093987878094...|\n",
      "|154514|         LAS FUENTES|         LAS FUENTES|  12575|          Restaurant|  Risk 1 (High)|  2558 N HALSTED ST |CHICAGO| IL|60614|09/21/2010|             Canvass|              Fail|18. NO EVIDENCE O...|  41.9290354100918|-87.64903392789199|(41.9290354100918...|\n",
      "|413711|CASA CENTRAL COMM...|CASA CENTRAL COMM...|  60766|          Restaurant|  Risk 1 (High)|1343 N CALIFORNIA...|CHICAGO| IL|60622|09/21/2010|             License|              Pass|41. PREMISES MAIN...| 41.90598597077873|-87.69680735572291|(41.9059859707787...|\n",
      "|413764|LA BRUQUENA RESTA...|LA BRUQUENA RESTA...|1492868|          Restaurant|  Risk 1 (High)| 2726 W DIVISION ST |CHICAGO| IL|60622|09/21/2010|Suspected Food Po...|Pass w/ Conditions|4. SOURCE OF CROS...|41.903046386818346|  -87.695535129416|(41.9030463868183...|\n",
      "|413732|SODEXHO AT UNITED...|SODEXHO AT UNITED...|  20467|          Restaurant|  Risk 1 (High)|  11601 W TOUHY AVE |CHICAGO| IL|60666|09/21/2010|             Canvass|   Out of Business|                null|42.008536400868735|-87.91442843927047|(42.0085364008687...|\n",
      "|413757|WHIZ KIDS NURSERY...|WHIZ KIDS NURSERY...|1948277|Daycare Above and...|  Risk 1 (High)| 514-522 W 103RD ST |CHICAGO| IL|60628|09/21/2010|             Canvass|              Pass|35. WALLS, CEILIN...|41.707112812685075|-87.63620425242559|(41.7071128126850...|\n",
      "|363272|FRUTERIA GUAYAUIT...|FRUTERIA GUAYAUIT...|1446823|       Grocery Store|   Risk 3 (Low)|  3849 S KEDZIE AVE |CHICAGO| IL|60632|09/21/2010|             Canvass|   Out of Business|                null| 41.82290845958193|-87.70426021024545|(41.8229084595819...|\n",
      "+------+--------------------+--------------------+-------+--------------------+---------------+--------------------+-------+---+-----+----------+--------------------+------------------+--------------------+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspections.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the CSV file as a DataFrame. It has some columns we will not use. Dropping them can save memory when caching the DataFrame. Also, we should give these columns meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------------------+--------------------+\n",
      "|    id|                name|           results|          violations|\n",
      "+------+--------------------+------------------+--------------------+\n",
      "|413707|       LUNA PARK INC|              Fail|24. DISH WASHING ...|\n",
      "|391234|       CAFE SELMARIE|              Fail|2. FACILITIES TO ...|\n",
      "|413751|          MANCHU WOK|              Pass|33. FOOD AND NON-...|\n",
      "|120580|          SUSHI PINK|              Pass|32. FOOD AND NON-...|\n",
      "|413715|              NABO'S|              Pass|19. OUTSIDE GARBA...|\n",
      "|420207|  WHOLE FOODS MARKET|              Pass|32. FOOD AND NON-...|\n",
      "|154514|         LAS FUENTES|              Fail|18. NO EVIDENCE O...|\n",
      "|413711|CASA CENTRAL COMM...|              Pass|41. PREMISES MAIN...|\n",
      "|413764|LA BRUQUENA RESTA...|Pass w/ Conditions|4. SOURCE OF CROS...|\n",
      "|413757|WHIZ KIDS NURSERY...|              Pass|35. WALLS, CEILIN...|\n",
      "|154516|TACO & BURRITO PA...|              Pass|30. FOOD IN ORIGI...|\n",
      "|413759|  MARISCOS EL VENENO|              Pass|18. NO EVIDENCE O...|\n",
      "|114554|THE HANGGE- UPPE,...|              Pass|18. NO EVIDENCE O...|\n",
      "|413758|   LINDY'S CHILI INC|Pass w/ Conditions|30. FOOD IN ORIGI...|\n",
      "|343362|        FUMARE MEATS|              Pass|40. REFRIGERATION...|\n",
      "|413754|              Subway|              Pass|38. VENTILATION: ...|\n",
      "|289222|LITTLE CAESARS PIZZA|              Pass|34. FLOORS: CONST...|\n",
      "|413755|       BILLY'S GRILL|              Pass|33. FOOD AND NON-...|\n",
      "|343364|             FRESHII|              Fail|18. NO EVIDENCE O...|\n",
      "|289221|NICKY'S GRILL & Y...|              Pass|33. FOOD AND NON-...|\n",
      "+------+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10469"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unused columns and rename interesting columns.\n",
    "\n",
    "# Keep interesting columns and rename them to something meaningful\n",
    "\n",
    "# Mapping column index to name.\n",
    "columnNames = {0: \"id\", 1: \"name\", 12: \"results\", 13: \"violations\"}\n",
    "    \n",
    "# Rename column from '_c{id}' to something meaningful.\n",
    "cols = [inspections[i].alias(columnNames[i]) for i in columnNames.keys()]\n",
    "   \n",
    "# Drop columns we are not using.\n",
    "df = inspections.select(cols).where(col('violations').isNotNull())\n",
    "\n",
    "df.cache()\n",
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=413707, name='LUNA PARK INC', results='Fail', violations='24. DISH WASHING FACILITIES: PROPERLY DESIGNED, CONSTRUCTED, MAINTAINED, INSTALLED, LOCATED AND OPERATED - Comments: All dishwashing machines must be of a type that complies with all requirements of the plumbing section of the Municipal Code of Chicago and Rules and Regulation of the Board of Health. OBSEVERD THE 3 COMPARTMENT SINK BACKING UP INTO THE 1ST AND 2ND COMPARTMENT WITH CLEAR WATER AND SLOWLY DRAINING OUT. INST NEED HAVE IT REPAIR. CITATION ISSUED, SERIOUS VIOLATION 7-38-030 H000062369-10 COURT DATE 10-28-10 TIME 1 P.M. ROOM 107 400 W. SURPERIOR. | 36. LIGHTING: REQUIRED MINIMUM FOOT-CANDLES OF LIGHT PROVIDED, FIXTURES SHIELDED - Comments: Shielding to protect against broken glass falling into food shall be provided for all artificial lighting sources in preparation, service, and display facilities. LIGHT SHIELD ARE MISSING UNDER HOOD OF  COOKING EQUIPMENT AND NEED TO REPLACE LIGHT UNDER UNIT. 4 LIGHTS ARE OUT IN THE REAR CHILDREN AREA,IN THE KINDERGARDEN CLASS ROOM. 2 LIGHT ARE OUT EAST REAR, LIGHT FRONT WEST ROOM. NEED TO REPLACE ALL LIGHT THAT ARE NOT WORKING. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: The walls and ceilings shall be in good repair and easily cleaned. MISSING CEILING TILES WITH STAINS IN WEST,EAST, IN FRONT AREA WEST, AND BY THE 15MOS AREA. NEED TO BE REPLACED. | 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: All food and non-food contact equipment and utensils shall be smooth, easily cleanable, and durable, and shall be in good repair. SPLASH GUARDED ARE NEEDED BY THE EXPOSED HAND SINK IN THE KITCHEN AREA | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: The floors shall be constructed per code, be smooth and easily cleaned, and be kept clean and in good repair. INST NEED TO ELEVATE ALL FOOD ITEMS 6INCH OFF THE FLOOR 6 INCH AWAY FORM WALL.  ')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above cell gives us an idea of the schema of the input file; the file includes the name of every establishment, the type of establishment, the address, the data of the inspections, and the location, among other things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start to get a sense of what our dataset contains. For example, what are the different values in the `results` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|           results|\n",
      "+------------------+\n",
      "|              Fail|\n",
      "|Pass w/ Conditions|\n",
      "|              Pass|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('results').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|           results|count|\n",
      "+------------------+-----+\n",
      "|              Fail| 2607|\n",
      "|Pass w/ Conditions| 1028|\n",
      "|              Pass| 6834|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('results').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us develop a model that can guess the outcome of a food inspection, given the violations. Since logistic regression is a binary classification method, it makes sense to group our data into two categories: **Fail** and **Pass**. A \"Pass w/ Conditions\" is still a Pass, so when we train the model, we will consider the two results equivalent.\n",
    "\n",
    "Let us go ahead and convert our existing dataframe(`df`) into a new dataframe where each inspection is represented as a label-violations pair. In our case, a label of 0 represents a failure, a label of 1 represents a success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|          violations|\n",
      "+-----+--------------------+\n",
      "|    0|24. DISH WASHING ...|\n",
      "|    0|2. FACILITIES TO ...|\n",
      "|    1|33. FOOD AND NON-...|\n",
      "|    1|32. FOOD AND NON-...|\n",
      "|    1|19. OUTSIDE GARBA...|\n",
      "|    1|32. FOOD AND NON-...|\n",
      "|    0|18. NO EVIDENCE O...|\n",
      "|    1|41. PREMISES MAIN...|\n",
      "|    1|4. SOURCE OF CROS...|\n",
      "|    1|35. WALLS, CEILIN...|\n",
      "|    1|30. FOOD IN ORIGI...|\n",
      "|    1|18. NO EVIDENCE O...|\n",
      "|    1|18. NO EVIDENCE O...|\n",
      "|    1|30. FOOD IN ORIGI...|\n",
      "|    1|40. REFRIGERATION...|\n",
      "|    1|38. VENTILATION: ...|\n",
      "|    1|34. FLOORS: CONST...|\n",
      "|    1|33. FOOD AND NON-...|\n",
      "|    0|18. NO EVIDENCE O...|\n",
      "|    1|33. FOOD AND NON-...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The function to clean the data\n",
    "\n",
    "labeledData = df.select(when(df.results == 'Fail', 0)\n",
    "                        .when(df.results == 'Pass', 1)\n",
    "                        .otherwise(1)\n",
    "                        .alias('label'), \n",
    "                        'violations')\n",
    "labeledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model from the input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|          violations|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    0|1. SOURCE SOUND C...|[1., source, soun...|(262144,[1972,278...|[1.48422506159956...|[0.81520990329150...|       0.0|\n",
      "|    0|1. SOURCE SOUND C...|[1., source, soun...|(262144,[4089,963...|[6.85827341414378...|[0.99895037633049...|       0.0|\n",
      "|    0|10. SEWAGE AND WA...|[10., sewage, and...|(262144,[4640,487...|[10.2233134272921...|[0.99996368754419...|       0.0|\n",
      "|    0|10. SEWAGE AND WA...|[10., sewage, and...|(262144,[2786,306...|[4.58834123777139...|[0.98993266835200...|       0.0|\n",
      "|    0|10. SEWAGE AND WA...|[10., sewage, and...|(262144,[1972,269...|[9.73410913028309...|[0.99994077509174...|       0.0|\n",
      "|    0|10. SEWAGE AND WA...|[10., sewage, and...|(262144,[1972,522...|[0.46101152616620...|[0.61325411081331...|       0.0|\n",
      "|    0|11. ADEQUATE NUMB...|[11., adequate, n...|(262144,[1536,187...|[7.94294301910621...|[0.99964496639668...|       0.0|\n",
      "|    0|11. ADEQUATE NUMB...|[11., adequate, n...|(262144,[2786,306...|[4.03645487818248...|[0.98264649405375...|       0.0|\n",
      "|    0|12. HAND WASHING ...|[12., hand, washi...|(262144,[1836,197...|[5.86678448296939...|[0.99717603243656...|       0.0|\n",
      "|    0|12. HAND WASHING ...|[12., hand, washi...|(262144,[523,2106...|[0.90322110814328...|[0.71161099129078...|       0.0|\n",
      "|    0|12. HAND WASHING ...|[12., hand, washi...|(262144,[2106,278...|[6.42909150659583...|[0.99838868412805...|       0.0|\n",
      "|    0|12. HAND WASHING ...|[12., hand, washi...|(262144,[215,1836...|[3.31041457720888...|[0.96478436897247...|       0.0|\n",
      "|    0|12. HAND WASHING ...|[12., hand, washi...|(262144,[523,1972...|[-3.6477249733905...|[0.02538893651289...|       1.0|\n",
      "|    0|14. PREVIOUS SERI...|[14., previous, s...|(262144,[3298,453...|[-4.3654527872658...|[0.01254941025148...|       1.0|\n",
      "|    0|16. FOOD PROTECTE...|[16., food, prote...|(262144,[2786,316...|[4.45128192981653...|[0.98847086581340...|       0.0|\n",
      "|    0|16. FOOD PROTECTE...|[16., food, prote...|(262144,[1189,278...|[0.32239924560503...|[0.57990885514704...|       0.0|\n",
      "|    0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[1836,234...|[-0.9884660945902...|[0.27121515913288...|       1.0|\n",
      "|    0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[2786,306...|[6.47124151371987...|[0.99845508690811...|       0.0|\n",
      "|    0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[1463,278...|[4.70023713429343...|[0.99098881919808...|       0.0|\n",
      "|    0|18. NO EVIDENCE O...|[18., no, evidenc...|(262144,[1972,278...|[1.92019615648079...|[0.87216030607915...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData, testData = labeledData.randomSplit([0.8, 0.2])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"violations\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\") \n",
    "# HashingTF的功能是将tokenizer转换为features向量\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "predictionsDf = model.transform(testData)\n",
    "predictionsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 2134 inspections and there were 1868 successful predictions\n",
      "This is a 87% success rate\n"
     ]
    }
   ],
   "source": [
    "numSuccesses = predictionsDf.where('label == prediction').count()\n",
    "numInspections = predictionsDf.count()\n",
    "\n",
    "print (\"There were %d inspections and there were %d successful predictions\" % (numInspections, numSuccesses))\n",
    "print(\"This is a %d%% success rate\" % (float(numSuccesses) / float(numInspections) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation（交叉验证）\n",
    "\n",
    "CrossValidator begins by splitting the dataset into a set of folds which are used as separate training and test datasets. E.g., with k=5 folds, CrossValidator will generate 5 (training, test) dataset pairs, each of which uses 4/5 of the data for training and 1/5 for testing. To evaluate a particular ParamMap, CrossValidator computes the average evaluation metric for the 5 Models produced by fitting the Estimator on the 5 different (training, test) dataset pairs.\n",
    "\n",
    "After identifying the best ParamMap, CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-33-cf5e8db38af9>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-cf5e8db38af9>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\ \n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\        \n",
    "    .build()\n",
    "\n",
    "# 有三个备选项\n",
    "# 有两个备选项\n",
    "# crossval会自动尝试所有的combination of parameters\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)  \n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(trainingData)\n",
    "\n",
    "predictionsDf = cvModel.transform(testData)\n",
    "\n",
    "numSuccesses = predictionsDf.where('label == prediction').count()\n",
    "numInspections = predictionsDf.count()\n",
    "\n",
    "print (\"There were %d inspections and there were %d successful predictions\" % (numInspections, numSuccesses))\n",
    "print(\"This is a %d%% success rate\" % (float(numSuccesses) / float(numInspections) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimator: estimator to be cross-validated (current: Pipeline_4d508b7dc95479bf7897)\n",
       "estimatorParamMaps: estimator param maps (current: [{Param(parent=u'LogisticRegression_4de3adfa7c180a25b4d2', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent=u'HashingTF_4bf9b60fc08483f17273', name='numFeatures', doc='number of features.'): 10}, {Param(parent=u'LogisticRegression_4de3adfa7c180a25b4d2', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent=u'HashingTF_4bf9b60fc08483f17273', name='numFeatures', doc='number of features.'): 100}, {Param(parent=u'LogisticRegression_4de3adfa7c180a25b4d2', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent=u'HashingTF_4bf9b60fc08483f17273', name='numFeatures', doc='number of features.'): 1000}, {Param(parent=u'LogisticRegression_4de3adfa7c180a25b4d2', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent=u'HashingTF_4bf9b60fc08483f17273', name='numFeatures', doc='number of features.'): 10}, {Param(parent=u'LogisticRegression_4de3adfa7c180a25b4d2', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent=u'HashingTF_4bf9b60fc08483f17273', name='numFeatures', doc='number of features.'): 100}, {Param(parent=u'LogisticRegression_4de3adfa7c180a25b4d2', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent=u'HashingTF_4bf9b60fc08483f17273', name='numFeatures', doc='number of features.'): 1000}])\n",
       "evaluator: evaluator used to select hyper-parameters that maximize the validator metric (current: BinaryClassificationEvaluator_416488c06eaaa8f8fdc2)\n",
       "seed: random seed. (default: -4372709618522015412)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {
    "6e9e93a4c6b04588a79d6ee36f392422": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    },
    "cd9a9b22fd1641b7a624fba5271e8b86": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
