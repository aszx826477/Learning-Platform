{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I make an RDD?\n",
    "\n",
    "RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from files on the local drive.  All data files can be downloaded from https://www.cse.ust.hk/msbd5003/data/\n",
    "\n",
    "RDD is short for *Resilient Distributed Dataset*\n",
    "\n",
    "\n",
    "An RDD is, essentially, the Spark representation of a set of data, spread across multiple machines, with APIs to let you act on it. An RDD could come from any datasource, e.g. text files, a database via JDBC, etc.\n",
    "\n",
    "The formal definition is:\n",
    "\n",
    "    RDDs are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators.\n",
    "\n",
    "If you want the full details on what an RDD is, read one of the core Spark academic papers, Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark modules\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://DESKTOP-UCGP8OJ:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        ",
      "text/plain": "<SparkContext master=local[*] appName=pyspark-shell>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['22222apple', 'banana', 'canary melon', 'grape', 'lemon', 'orange', 'pineapple', 'strawberry']\n['banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower']\n"
    }
   ],
   "source": [
    "# Read data from local file system:\n",
    "fruits = sc.textFile('data/fruits.txt')\n",
    "yellowThings = sc.textFile('data/yellowthings.txt')\n",
    "print (fruits.collect())\n",
    "print (yellowThings.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.lang.IllegalArgumentException: java.net.UnknownHostException: url\n\tat org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:378)\n\tat org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:310)\n\tat org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176)\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:678)\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:619)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.UnknownHostException: url\n\t... 44 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-995600000321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read data from HDFS :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfruits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hdfs://url:9000/pathname/fruits.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfruits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.lang.IllegalArgumentException: java.net.UnknownHostException: url\n\tat org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:378)\n\tat org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:310)\n\tat org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:176)\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:678)\n\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:619)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.UnknownHostException: url\n\t... 44 more\n"
     ]
    }
   ],
   "source": [
    "# Read data from HDFS :\n",
    "# You need to open Hadoop HDFS\n",
    "fruits = sc.textFile('hdfs://url:9000/pathname/fruits.txt')\n",
    "fruits.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "##  RDD operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "```\n",
    "fruits = ['badapple', 'banana', 'canary melon', 'grape', 'lemon', 'orange', 'pineapple', 'strawberry']\n",
    "YellowThings = ['banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "fruitsReversed = fruits.map(lambda fruit: fruit[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elppa22222',\n",
       " 'ananab',\n",
       " 'nolem yranac',\n",
       " 'eparg',\n",
       " 'nomel',\n",
       " 'egnaro',\n",
       " 'elppaenip',\n",
       " 'yrrebwarts']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fruitsReversed = fruits.map(lambda fruit: fruit[::-1])\n",
    "# fruitsReversed.cache() # .cache()\n",
    "# try changing the file and re-execute with and without cache\n",
    "fruitsReversed.collect()\n",
    "\n",
    "\n",
    "# File ---> fruits ---> fruitReversed\n",
    "#              |------>\n",
    "#   |----->\n",
    "# see a chain sequence relationship, and we also need to know about 're-assigned'\n",
    "# if we don't use cache(), we will re-calculate along the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grape', 'lemon']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter\n",
    "shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5)\n",
    "shortFruits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'a',\n",
       " 'd',\n",
       " 'a',\n",
       " 'p',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " 'b',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'c',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'r',\n",
       " 'y',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " 'l',\n",
       " 'o',\n",
       " 'n',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'p',\n",
       " 'e',\n",
       " 'l',\n",
       " 'e',\n",
       " 'm',\n",
       " 'o',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 'g',\n",
       " 'e',\n",
       " 'p',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e',\n",
       " 'a',\n",
       " 'p',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'r',\n",
       " 'a',\n",
       " 'w',\n",
       " 'b',\n",
       " 'e',\n",
       " 'r',\n",
       " 'r',\n",
       " 'y']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap\n",
    "characters = fruits.flatMap(lambda fruit: list(fruit))\n",
    "characters.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['badapple',\n",
       " 'banana',\n",
       " 'canary melon',\n",
       " 'grape',\n",
       " 'lemon',\n",
       " 'orange',\n",
       " 'pineapple',\n",
       " 'strawberry',\n",
       " 'banana',\n",
       " 'bee',\n",
       " 'butter',\n",
       " 'canary melon',\n",
       " 'gold',\n",
       " 'lemon',\n",
       " 'pineapple',\n",
       " 'sunflower']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union\n",
    "fruitsAndYellowThings = fruits.union(yellowThings)\n",
    "fruitsAndYellowThings.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "```\n",
    "fruits = ['badapple', 'banana', 'canary melon', 'grape', 'lemon', 'orange', 'pineapple', 'strawberry']\n",
    "YellowThings = ['banana', 'bee', 'butter', 'canary melon', 'gold', 'lemon', 'pineapple', 'sunflower']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pineapple', 'canary melon', 'lemon', 'banana']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intersection\n",
    "# find the same elements between two sets\n",
    "yellowFruits = fruits.intersection(yellowThings)\n",
    "yellowFruits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orange',\n",
       " 'pineapple',\n",
       " 'canary melon',\n",
       " 'grape',\n",
       " 'lemon',\n",
       " 'bee',\n",
       " 'badapple',\n",
       " 'banana',\n",
       " 'butter',\n",
       " 'gold',\n",
       " 'sunflower',\n",
       " 'strawberry']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct\n",
    "# drop out the same elements\n",
    "distinctFruitsAndYellowThings = fruitsAndYellowThings.distinct()\n",
    "distinctFruitsAndYellowThings.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD actions\n",
    "Following are examples of some of the common actions available. For a detailed list, see [RDD Actions](https://spark.apache.org/docs/2.0.0/programming-guide.html#actions).\n",
    "\n",
    "Run some transformations below to understand this better. Place the cursor in the cell and press **SHIFT + ENTER**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['badapple',\n",
       " 'banana',\n",
       " 'canary melon',\n",
       " 'grape',\n",
       " 'lemon',\n",
       " 'orange',\n",
       " 'pineapple',\n",
       " 'strawberry']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect\n",
    "fruitsArray = fruits.collect()\n",
    "yellowThingsArray = yellowThings.collect()\n",
    "fruitsArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "numFruits = fruits.count()\n",
    "numFruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['badapple', 'banana', 'canary melon']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take\n",
    "first3Fruits = fruits.take(3)\n",
    "first3Fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'g',\n",
       " 'i',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'w',\n",
       " 'y'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce\n",
    "letterSet = fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y))\n",
    "letterSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'd',\n",
       " 'p',\n",
       " 'l',\n",
       " 'c',\n",
       " 'r',\n",
       " 'y',\n",
       " 'g',\n",
       " 'i',\n",
       " 's',\n",
       " 'a',\n",
       " 'e',\n",
       " 'n',\n",
       " ' ',\n",
       " 'm',\n",
       " 'o',\n",
       " 't',\n",
       " 'w']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letterSet = fruits.flatMap(lambda fruit: list(fruit)).distinct().collect()\n",
    "letterSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure (闭包)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "increment = 10\n",
    "rdd = sc.parallelize(range(10)) # python3 xrange() and range() merge into range()\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "\n",
    "print (rdd.collect())\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print (counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "accum = sc.accumulator(0) # which means 累加器\n",
    "\n",
    "def g(x):\n",
    "    global accum\n",
    "    accum += x\n",
    "\n",
    "a = rdd.foreach(g)\n",
    "\n",
    "print (accum.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9 34\n",
      "10 11 35\n",
      "21 24 45\n",
      "a = 45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint (\"accum.value = %s\" % accum.value)\\nprint (\"accum = %s\" % accum)\\nprint (rdd.reduce(lambda x, y: x+y))\\n#a.cache()\\ntmp = a.count()\\nprint (\"accum.value = %s\" % accum.value)\\nprint (rdd.reduce(lambda x, y: x+y))\\n\\ntmp = a.count()\\nprint (\"accum.value = %s\" % accum.value)\\nprint (rdd.reduce(lambda x, y: x+y))\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "def g(x):\n",
    "    global accum\n",
    "    print(x, accum)\n",
    "    accum += x\n",
    "    return x * x\n",
    "\n",
    "def h(x,y):\n",
    "    global accum\n",
    "    print(x,y,accum)\n",
    "    accum += x\n",
    "    return x + y\n",
    "\n",
    "# a = rdd.map(g)\n",
    "# print (a.collect())\n",
    "a = rdd.reduce(h)\n",
    "print(\"a = %s\" % a)\n",
    "\n",
    "'''\n",
    "print (\"accum.value = %s\" % accum.value)\n",
    "print (\"accum = %s\" % accum)\n",
    "print (rdd.reduce(lambda x, y: x+y))\n",
    "#a.cache()\n",
    "tmp = a.count()\n",
    "print (\"accum.value = %s\" % accum.value)\n",
    "print (rdd.reduce(lambda x, y: x+y))\n",
    "\n",
    "tmp = a.count()\n",
    "print (\"accum.value = %s\" % accum.value)\n",
    "print (rdd.reduce(lambda x, y: x+y))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Pi using Monte Carlo simulation(蒙特卡洛模拟)\n",
    "\n",
    "\n",
    "$$ \\frac{\\pi}{4}\\qquad = \\frac{n_{circle}}{n}\\qquad $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.036\n"
     ]
    }
   ],
   "source": [
    "# From the official spark examples.\n",
    "\n",
    "import sys\n",
    "import random\n",
    "\n",
    "partitions = 1000 # one worker is responsible for a partition\n",
    "n = 1000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random.random()\n",
    "    y = random.random()\n",
    "    return 1 if x ** 2 + y ** 2 < 1 else 0 # 1 means in the circle, 0 means out of the circle\n",
    "\n",
    "count = sc.parallelize(range(1, n + 1), partitions) \\\n",
    "          .map(f).sum()\n",
    "\n",
    "print (\"Pi is roughly\", 4.0 * count / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "\n",
      "\n",
      "[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]\n",
      "\n",
      "\n",
      "[[0.041191522902819355, 0.08276950396564542, 0.8103453157633747, 0.6501418010304806], [0.041191522902819355, 0.08276950396564542, 0.8103453157633747, 0.6501418010304806], [0.041191522902819355, 0.08276950396564542, 0.8103453157633747, 0.6501418010304806], [0.041191522902819355, 0.08276950396564542, 0.8103453157633747, 0.6501418010304806], [0.041191522902819355, 0.08276950396564542, 0.8103453157633747, 0.6501418010304806]]\n"
     ]
    }
   ],
   "source": [
    "# Example: glom\n",
    "import sys\n",
    "import random\n",
    "a = sc.parallelize(range(0,20),5)\n",
    "print (a.collect())\n",
    "print (\"\\n\")\n",
    "print (a.glom().collect())\n",
    "print (\"\\n\")\n",
    "print (a.map(lambda x: random.random()).glom().collect()) \n",
    "# the num is as the seed to create random values\n",
    "# the same seed will produce the same sequence of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: What is 'yield' in python?\n",
    "\n",
    "A generator ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]\n",
      "[10, 35, 60, 85]\n",
      "[0, 1, 3, 6, 10, 6, 12, 19, 27, 36, 12, 23, 35, 48, 62, 18, 34, 51, 69, 88]\n"
     ]
    }
   ],
   "source": [
    "# Example: mapPartition and mapPartitionWithIndex\n",
    "a = sc.parallelize(range(0,20),4)\n",
    "print (a.glom().collect())\n",
    "\n",
    "def f(it):\n",
    "    s = 0\n",
    "    for i in it:\n",
    "        s += i\n",
    "    yield s\n",
    "\n",
    "print (a.mapPartitions(f).collect())\n",
    "\n",
    "def f(index, it):\n",
    "    s = index\n",
    "    for i in it:\n",
    "        s += i\n",
    "        yield s\n",
    "\n",
    "print (a.mapPartitionsWithIndex(f).collect())\n",
    "# 0 = 0 + 0 (index = 0)\n",
    "# 6 = 5 + 1 (index = 1)\n",
    "# 12 = 10 + 2 (index = 2)\n",
    "# 18 = 15 + 3 (index = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.138732\n"
     ]
    }
   ],
   "source": [
    "# Correct version\n",
    "import random\n",
    "\n",
    "partitions = 1000\n",
    "n = 1000 * partitions\n",
    "\n",
    "def f(index, it):\n",
    "    random.seed(index + 987236) # avoid different patitions using the same seed sequence\n",
    "    for i in it:\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        yield 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "count = sc.parallelize(range(1, n + 1), partitions) \\\n",
    "          .mapPartitionsWithIndex(f).sum()\n",
    "\n",
    "print (\"Pi is roughly\", 4.0 * count / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure and Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The difference between cache() and persist()**\n",
    "\n",
    "通过源码可以看出cache()是persist()的简化方式，调用persist的无参版本，也就是调用persist(StorageLevel.MEMORY_ONLY)，cache只有一个默认的缓存级别MEMORY_ONLY，即将数据持久化到内存中，而persist可以通过传递一个 StorageLevel 对象来设置缓存的存储级别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[0, 1, 2, 3, 4]\n",
      "3\n",
      "[0, 1, 2]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "A = sc.parallelize(range(10))\n",
    "\n",
    "x = 5\n",
    "B = A.filter(lambda z: z < x)\n",
    "# B.cache()\n",
    "print (B.count())\n",
    "print (B.collect())\n",
    "# print B.collect()\n",
    "x = 3\n",
    "print(B.count())\n",
    "print (B.take(B.count())) \n",
    "print (B.collect())\n",
    "# print B.collect()\n",
    "# collect() doesn't always re-collect data - bad design!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD variables are references\n",
    "A = sc.parallelize(range(10))\n",
    "print(A.take(10))\n",
    "B = A.map(lambda x: x*2)\n",
    "A = B.map(lambda x: x+1)\n",
    "A.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A =  [34, 67, 21, 56, 47, 89, 12, 44, 74, 43, 26]\n",
      "-----------do A.first()-------------\n",
      "x =  34\n",
      "A =  [34, 67, 21, 56, 47, 89, 12, 44, 74, 43, 26]\n",
      "-----------do A.filter()-------------\n",
      "A1 =  [21, 12, 26]\n",
      "A2 =  [67, 56, 47, 89, 44, 74, 43]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------next loop-------------\n",
      "A =  [67, 56, 47, 89, 44, 74, 43]\n",
      "A1 =  [21, 12, 26]\n",
      "A2 =  [67, 56, 47, 89, 44, 74, 43]\n",
      "-----------do A.first()-------------\n",
      "x =  67\n",
      "A =  [89, 74]\n",
      "A1 =  [34, 21, 56, 47, 12, 44, 43, 26]\n",
      "A2 =  [89, 74]\n",
      "-----------do A.filter()-------------\n",
      "A1 =  []\n",
      "A2 =  [89, 74]\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "# Linear-time selection\n",
    "\n",
    "data = [34, 67, 21, 56, 47, 89, 12, 44, 74, 43, 26]\n",
    "# 12 21 26 34 43 44 47 56 67 74 89\n",
    "\n",
    "A = sc.parallelize(data,2) # slice = 2\n",
    "A1 = []\n",
    "A2 = []\n",
    "# print(A.collect()）\n",
    "k = 4 # find the element with index = 4 from small to big---> 43 (index begin from 0)\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(\"A = \", A.take(A.count()))\n",
    "    if i >= 2:\n",
    "        print(\"A1 = \", A1.take(A1.count()))\n",
    "        print(\"A2 = \", A2.take(A2.count()))\n",
    "    x = A.first()\n",
    "    print(\"-----------do A.first()-------------\")\n",
    "    print(\"x = \", x)\n",
    "    print(\"A = \", A.take(A.count()))\n",
    "    if i >= 2:\n",
    "        print(\"A1 = \", A1.take(A1.count()))\n",
    "        print(\"A2 = \", A2.take(A2.count()))\n",
    "    A1 = A.filter(lambda z: z < x)\n",
    "    A2 = A.filter(lambda z: z > x)\n",
    "    print(\"-----------do A.filter()-------------\")\n",
    "    print(\"A1 = \", A1.take(A1.count()))\n",
    "    print(\"A2 = \", A2.take(A2.count()))\n",
    "    mid = A1.count()\n",
    "    if mid == k:\n",
    "        print (x)\n",
    "        break\n",
    "    \n",
    "    if k < mid:\n",
    "        A = A1\n",
    "    else:\n",
    "        A = A2\n",
    "        k = k - mid - 1\n",
    "    # A.cache()\n",
    "    i = i + 1\n",
    "    print(\"\\n\\n\\n\\n---------next loop-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear-time selection\n",
    "\n",
    "data = [34, 67, 21, 56, 47, 89, 12, 44, 74, 43, 26]\n",
    "# 12 21 26 34 43 44 47 56 67 74 89\n",
    "\n",
    "A = sc.parallelize(data,2) # slice = 2\n",
    "# print(A.collect()）\n",
    "k = 4 # find the element with index = 4 from small to big---> 43 (index begin from 0)\n",
    "\n",
    "while True:\n",
    "   \n",
    "    x = A.first()\n",
    "    \n",
    "    A1 = A.filter(lambda z: z < x)\n",
    "    A2 = A.filter(lambda z: z > x)\n",
    "    \n",
    "    mid = A1.count()\n",
    "    if mid == k:\n",
    "        print (x)\n",
    "        break\n",
    "    if k < mid:\n",
    "        A = A1\n",
    "    else:\n",
    "        A = A2\n",
    "        k = k - mid - 1\n",
    "    # A.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要理解这个问题，首先要明白两个问题：\n",
    "* 继承（也叫依赖）\n",
    "* 重计算\n",
    "\n",
    "我们在编程中常认为`=`运算是赋值的意思，但在RDD中这里面除了赋值，还有一层继承的关系，何为**继承**呢？我们先看下面一个简单的例子：\n",
    "```\n",
    "data = [2, 5, 9, 7, 8, 1, 3, 4]\n",
    "x = 5\n",
    "A = sc.parallelize(data,2)\n",
    "A1 = A.filter(lambda z: z < x)\n",
    "A2 = A.filter(lambda z: z > x)\n",
    "```\n",
    "A可以认为是一个父结点，它是一个RDD对象，在代码第4,5行，我们使用赋值语句时，会创建两个新的RDD对象，并且构建如下图的依赖关系，我们可以说A1, A2是从父结点A继承而来的。\n",
    "```\n",
    "A = [2,5,9,7,8,1,3,4] ---- |--(小于5)--> A1 = [2,1,3,4]\n",
    "                           |--(大于5)--> A2 = [9,7,8]\n",
    "```\n",
    "我们再运行下面的语句，又会发生什么呢？\n",
    "```\n",
    "A = A2\n",
    "```\n",
    "如果按照我们以前编程的知识，是不是认为A只是被重新赋了值，即`A = [9,7,8]`。但在RDD中，这会新创建一个RDD对象，如下图所示：\n",
    "```\n",
    "A = [2,5,9,7,8,1,3,4] ----- |--(小于5)--> A1 = [2,1,3,4]\n",
    "                            |--(大于5)--> A2 = [9,7,8] --(等于)--> A = [9,7,8]\n",
    "```\n",
    "如果我们再运行下面的代码，取出A中的第一个元素，这时会发生什么呢？这就要涉及到**重计算**的概念了。\n",
    "```\n",
    "x = A.first()\n",
    "```\n",
    "因为我们并没有运行`A.cache()`或`A.persist()`将A持久化，这就会导致在执行完`A.first()`得到`x = 9`后，还会按照上面的依赖关系从头到尾再计算一遍。由于`x`值的改变，会导致A1,A2发生变化，A1是小于9的集合，而A2变成了空集，进而导致A也变成了空集，但是最开始的那个父结点A并不受影响，虽然都是A但是是两个不同的RDD对象。\n",
    "```\n",
    "A = [2,5,9,7,8,1,3,4] ---- |--(小于9)--> A1 = [2,5,7,8,1,3,4]\n",
    "                           |--(大于9)--> A2 = [] --(等于)--> A = []\n",
    "```\n",
    "我们回到真正的问题上，为什么不用`A.cache()`会导致最终的输出是`x=67`，我们画出在第一次循环执行完的RDD-依赖链式关系图：\n",
    "```\n",
    "A = [34,67,21,56,47,89,12,44,74,43,26] ---- |--(小于34)--> A1 = [21,12,26]\n",
    "                                            |--(大于34)--> A2 = [67,56,47,89,44,74,43] \n",
    "                                                           |--(等于)--> A = [67,56,47,89,44,74,43]\n",
    "```\n",
    "由于没有运行`A.cache()`在第二次循环执行`A.first()`语句得到`x = 67`后，Spark在后面偷偷摸摸地重新计算，也就是\n",
    "```\n",
    "A = [34,67,21,56,47,89,12,44,74,43,26] ---- |--(小于67)--> A1 = [34,21,56,47,12,44,43,26]\n",
    "                                            |--(大于67)--> A2 = [89,74] \n",
    "                                                           |--(等于)--> A = [89,74]\n",
    "```\n",
    "这时候运行下面的代码\n",
    "```\n",
    "A1 = A.filter(lambda z: z < x)\n",
    "A2 = A.filter(lambda z: z > x)\n",
    "```\n",
    "会继续延展依赖关系，即\n",
    "```\n",
    "A = [34,67,21,56,47,89,12,44,74,43,26] ---- |--(小于67)--> A1 = [34,21,56,47,12,44,43,26]\n",
    "                                            |--(大于67)--> A2 = [89,74] \n",
    "                                                           |--(等于)--> A = [89,74]\n",
    "                                                                        |--(小于67)--> A1 = []\n",
    "                                                                        |--(大于67)--> A2 = [89,74] \n",
    "                                                                                     |--(等于)--> A = [89,74]\n",
    "```\n",
    "这时的`mid = A1.count()`应该会得到`0`等于k，所以输出`x = 67`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 21, 26, 34, 43, 44, 47, 56, 67, 74, 89]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process\n",
    "```\n",
    "x = 34\n",
    "A1 = 21 12 26\n",
    "A2 = 67 56 47 89 44 74 43\n",
    "mid = 3 < 4 = k\n",
    "A = A2 = 67 56 47 89 44 74 43\n",
    "k = 4 - 3 - 1 = 0\n",
    "A.cache()\n",
    "\n",
    "x = 67 # because A.cache ---> A = A2\n",
    "A1 = 56 47 44 43\n",
    "A2 = 89 74\n",
    "mid = 4 > 0 = k\n",
    "A = A1 = 56 47 44 43\n",
    "A.cache()\n",
    "\n",
    "x = 56\n",
    "A1 = 47 44 43\n",
    "A2 = \n",
    "mid = 3 > 0 = k\n",
    "A = A1 = 47 44 43\n",
    "A.cache()\n",
    "\n",
    "x = 47\n",
    "A1 = 44 43\n",
    "mid = 2 > 0 = k\n",
    "A = A1\n",
    "A.cache()\n",
    "\n",
    "x = 44\n",
    "A1 = 43\n",
    "mid = 1 > 0 = k\n",
    "A = A1\n",
    "A.cache()\n",
    "\n",
    "x = 43\n",
    "A1 = \n",
    "mid = 0 = k\n",
    "print(x)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key-Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 2), (12, 1), (10, 1), (9, 2), (5, 2)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey\n",
    "# reduceByKey，就是将key相同的键值对，按照Function进行计算。代码中就是将key相同的各value进行累加\n",
    "numFruitsByLength = fruits.map(lambda fruit: (len(fruit), 1)).reduceByKey(lambda x, y: x + y)\n",
    "numFruitsByLength.collect()\n",
    "# 6 letters fruits --- 2 nums\n",
    "# 12               --- 1 num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Course', 2),\n",
       " ('Information', 1),\n",
       " ('systems,', 1),\n",
       " ('cloud', 1),\n",
       " ('parallel', 1),\n",
       " ('as', 1),\n",
       " ('in', 2),\n",
       " ('mining', 1),\n",
       " ('massive', 1),\n",
       " ('amount', 1),\n",
       " ('of', 3),\n",
       " ('even', 1),\n",
       " ('servers', 1),\n",
       " ('centers.', 1),\n",
       " ('both', 1),\n",
       " ('hands-on', 1),\n",
       " ('this', 1),\n",
       " ('new', 1),\n",
       " ('Lecture', 1),\n",
       " ('videos', 1),\n",
       " ('Description', 1),\n",
       " ('Big', 1),\n",
       " ('data', 4),\n",
       " ('including', 1),\n",
       " ('computing', 1),\n",
       " ('and', 3),\n",
       " ('processing', 1),\n",
       " ('frameworks,', 1),\n",
       " ('emerge', 1),\n",
       " ('enabling', 1),\n",
       " ('technologies', 1),\n",
       " ('managing', 1),\n",
       " ('the', 2),\n",
       " ('across', 1),\n",
       " ('hundreds', 1),\n",
       " ('or', 1),\n",
       " ('thousands', 1),\n",
       " ('commodity', 1),\n",
       " ('This', 1),\n",
       " ('course', 1),\n",
       " ('exposes', 1),\n",
       " ('students', 1),\n",
       " ('to', 1),\n",
       " ('theory', 1),\n",
       " ('experience', 1),\n",
       " ('technology.', 1)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "lines = sc.textFile('data/course.txt')\n",
    "counts = lines.flatMap(lambda x: x.split()) \\\n",
    "              .map(lambda x: (x, 1)) \\\n",
    "              .reduceByKey(add)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 4),\n",
       " ('of', 3),\n",
       " ('and', 3),\n",
       " ('Course', 2),\n",
       " ('in', 2),\n",
       " ('the', 2),\n",
       " ('Information', 1),\n",
       " ('systems,', 1),\n",
       " ('cloud', 1),\n",
       " ('parallel', 1),\n",
       " ('as', 1),\n",
       " ('mining', 1),\n",
       " ('massive', 1),\n",
       " ('amount', 1),\n",
       " ('even', 1),\n",
       " ('servers', 1),\n",
       " ('centers.', 1),\n",
       " ('both', 1),\n",
       " ('hands-on', 1),\n",
       " ('this', 1),\n",
       " ('new', 1),\n",
       " ('Lecture', 1),\n",
       " ('videos', 1),\n",
       " ('Description', 1),\n",
       " ('Big', 1),\n",
       " ('including', 1),\n",
       " ('computing', 1),\n",
       " ('processing', 1),\n",
       " ('frameworks,', 1),\n",
       " ('emerge', 1),\n",
       " ('enabling', 1),\n",
       " ('technologies', 1),\n",
       " ('managing', 1),\n",
       " ('across', 1),\n",
       " ('hundreds', 1),\n",
       " ('or', 1),\n",
       " ('thousands', 1),\n",
       " ('commodity', 1),\n",
       " ('This', 1),\n",
       " ('course', 1),\n",
       " ('exposes', 1),\n",
       " ('students', 1),\n",
       " ('to', 1),\n",
       " ('theory', 1),\n",
       " ('experience', 1),\n",
       " ('technology.', 1)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.sortBy(lambda x: x[1], False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, ('Apple', (134, 'OK'))), (1, ('Apple', (135, 'OK'))), (1, ('Apple', (45, 'OK'))), (2, ('Orange', (53, 'OK'))), (3, ('TV', (34, 'OK'))), (5, ('Computer', (162, 'Error')))]\n"
     ]
    }
   ],
   "source": [
    "# Join simple example\n",
    "\n",
    "products = sc.parallelize([(1, \"Apple\"), (2, \"Orange\"), (3, \"TV\"), (5, \"Computer\")])\n",
    "#trans = sc.parallelize([(1, 134, \"OK\"), (3, 34, \"OK\"), (5, 162, \"Error\"), (1, 135, \"OK\"), (2, 53, \"OK\"), (1, 45, \"OK\")])\n",
    "trans = sc.parallelize([(1, (134, \"OK\")), (3, (34, \"OK\")), (5, (162, \"Error\")), (1, (135, \"OK\")), (2, (53, \"OK\")), (1, (45, \"OK\"))])\n",
    "\n",
    "print (products.join(trans).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final centers:  [array([0.1       , 0.33333333, 0.23333333]), array([9.05, 3.05, 4.65]), array([9.2, 2.2, 9.2])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parseVector(line):\n",
    "    return np.array([float(x) for x in line.split()])\n",
    "\n",
    "def closestPoint(p, centers):\n",
    "    bestIndex = 0\n",
    "    closest = float(\"+inf\")\n",
    "    for i in range(len(centers)):\n",
    "        tempDist = np.sum((p - centers[i]) ** 2)\n",
    "        if tempDist < closest:\n",
    "            closest = tempDist\n",
    "            bestIndex = i\n",
    "    return bestIndex\n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_data.txt\n",
    "lines = sc.textFile('data/kmeans_data.txt', 5)  \n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_bigdata.txt\n",
    "# lines = sc.textFile('data/kmeans_bigdata.txt', 5)  \n",
    "# lines is an RDD of strings\n",
    "K = 3\n",
    "convergeDist = 0.01  \n",
    "# terminate algorithm when the total distance from old center to new centers is less than this value\n",
    "\n",
    "data = lines.map(parseVector).cache() # data is an RDD of arrays\n",
    "\n",
    "kCenters = data.takeSample(False, K, 1)  # intial centers as a list of arrays\n",
    "tempDist = 1.0  # total distance from old centers to new centers\n",
    "\n",
    "while tempDist > convergeDist:\n",
    "    closest = data.map(lambda p: (closestPoint(p, kCenters), (p, 1)))\n",
    "    # for each point in data, find its closest center\n",
    "    # closest is an RDD of tuples (index of closest center, (point, 1))\n",
    "        \n",
    "    pointStats = closest.reduceByKey(lambda p1, p2: (p1[0] + p2[0], p1[1] + p2[1]))\n",
    "    # pointStats is an RDD of tuples (index of center,\n",
    "    # (array of sums of coordinates, total number of points assigned))\n",
    "    \n",
    "    newCenters = pointStats.map(lambda st: (st[0], st[1][0] / st[1][1])).collect()\n",
    "    # compute the new centers\n",
    "    \n",
    "    tempDist = sum(np.sum((kCenters[i] - p) ** 2) for (i, p) in newCenters)\n",
    "    # compute the total disctance from old centers to new centers\n",
    "    \n",
    "    for (i, p) in newCenters:\n",
    "        kCenters[i] = p\n",
    "        \n",
    "print (\"Final centers: \", kCenters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', <pyspark.resultiterable.ResultIterable object at 0x7f953cd930b8>), ('4', <pyspark.resultiterable.ResultIterable object at 0x7f953cd93b00>), ('2', <pyspark.resultiterable.ResultIterable object at 0x7f953cd932e8>), ('3', <pyspark.resultiterable.ResultIterable object at 0x7f953cd93128>)]\n",
      "4\n",
      "[('2', 0.5), ('3', 0.5), ('1', 1.0), ('3', 0.5), ('1', 0.5), ('4', 1.0)]\n",
      "[('1', 1.3125), ('4', 0.8875), ('2', 0.46249999999999997), ('3', 0.8875)]\n",
      "[('2', 0.65625), ('3', 0.65625), ('1', 0.8875), ('3', 0.23124999999999998), ('1', 0.23124999999999998), ('4', 0.8875)]\n",
      "[('1', 0.9884374999999999), ('4', 0.7918749999999999), ('2', 0.5953124999999999), ('3', 0.7918749999999999)]\n",
      "[('2', 0.49421874999999993), ('3', 0.49421874999999993), ('1', 0.7918749999999999), ('3', 0.29765624999999996), ('1', 0.29765624999999996), ('4', 0.7918749999999999)]\n",
      "[('1', 0.9636015624999998), ('4', 0.7105937499999999), ('2', 0.45758593749999993), ('3', 0.7105937499999999)]\n",
      "[('2', 0.4818007812499999), ('3', 0.4818007812499999), ('1', 0.7105937499999999), ('3', 0.22879296874999996), ('1', 0.22879296874999996), ('4', 0.7105937499999999)]\n",
      "[('1', 0.8359787109374999), ('4', 0.6415046874999999), ('2', 0.4470306640624999), ('3', 0.6415046874999999)]\n",
      "[('2', 0.41798935546874993), ('3', 0.41798935546874993), ('1', 0.6415046874999999), ('3', 0.22351533203124996), ('1', 0.22351533203124996), ('4', 0.6415046874999999)]\n",
      "[('1', 0.7727670166015623), ('4', 0.5827789843749999), ('2', 0.3927909521484374), ('3', 0.5827789843749999)]\n",
      "[('2', 0.38638350830078116), ('3', 0.38638350830078116), ('1', 0.5827789843749999), ('3', 0.1963954760742187), ('1', 0.1963954760742187), ('4', 0.5827789843749999)]\n",
      "[('1', 0.6997982913818357), ('4', 0.5328621367187498), ('2', 0.36592598205566396), ('3', 0.5328621367187498)]\n",
      "[('2', 0.34989914569091785), ('3', 0.34989914569091785), ('1', 0.5328621367187498), ('3', 0.18296299102783198), ('1', 0.18296299102783198), ('4', 0.5328621367187498)]\n",
      "[('1', 0.6459513585845945), ('4', 0.4904328162109373), ('2', 0.3349142738372801), ('3', 0.4904328162109373)]\n",
      "[('2', 0.32297567929229726), ('3', 0.32297567929229726), ('1', 0.4904328162109373), ('3', 0.16745713691864006), ('1', 0.16745713691864006), ('4', 0.4904328162109373)]\n",
      "[('1', 0.5967064601601407), ('4', 0.4543678937792967), ('2', 0.3120293273984526), ('3', 0.4543678937792967)]\n",
      "[('2', 0.29835323008007036), ('3', 0.29835323008007036), ('1', 0.4543678937792967), ('3', 0.1560146636992263), ('1', 0.1560146636992263), ('4', 0.4543678937792967)]\n",
      "[('1', 0.5563251738567445), ('4', 0.42371270971240216), ('2', 0.2911002455680598), ('3', 0.4237127097124021)]\n",
      "[('2', 0.27816258692837226), ('3', 0.27816258692837226), ('1', 0.42371270971240216), ('3', 0.1455501227840299), ('1', 0.1455501227840299), ('4', 0.4237127097124021)]\n",
      "[('1', 0.5213734076219673), ('4', 0.39765580325554173), ('2', 0.2739381988891164), ('3', 0.3976558032555418)]\n",
      "[('1', 0.5213734076219673), ('3', 0.3976558032555418), ('4', 0.39765580325554173), ('2', 0.2739381988891164)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from operator import add\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    # Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = urls.split(' ')\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "# Loads in input file. It should be in format of:\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/*\n",
    "lines = sc.textFile(\"data/pagerank_data.txt\", 2)\n",
    "# lines = sc.textFile(\"data/dblp.in\", 5)\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "# Loads all URLs from input file and initialize their neighbors. \n",
    "links = lines.map(lambda urls: parseNeighbors(urls)) \\\n",
    "             .groupByKey()\n",
    "\n",
    "print(links.collect())\n",
    "\n",
    "print(links.count())\n",
    "\n",
    "N = links.count()\n",
    "\n",
    "# Loads all URLs with other URL(s) link to from input file \n",
    "# and initialize ranks of them to one.\n",
    "ranks = links.mapValues(lambda neighbors: 1.0) # initialize to 1.0\n",
    "# ranks = links.map(lambda x: x[1] = 1.0)  less efficency\n",
    "\n",
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "for iteration in range(numOfIterations):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = links.join(ranks) \\\n",
    "                    .flatMap(lambda url_urls_rank:\n",
    "                             computeContribs(url_urls_rank[1][0],\n",
    "                                             url_urls_rank[1][1]))\n",
    "    # After the join, each element in the RDD is of the form\n",
    "    # (url, (list of neighbor urls, rank))\n",
    "    \n",
    "    # Re-calculates URL ranks based on neighbor contributions.\n",
    "    print(contribs.collect())\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15 / N)\n",
    "    # 噢，我懂了，这里是先reduceByKey,然后才对reduce后的RDD的rank的值*0.85，所以这里的rank已经是求和过的\n",
    "    print(ranks.collect())\n",
    "    # ranks = contribs.reduceByKey(add).map(lambda (url, rank): (url, rank * 0.85 + 0.15))\n",
    "\n",
    "# Use a huge linear graph\n",
    "\n",
    "print (ranks.top(5, lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', <pyspark.resultiterable.ResultIterable at 0x7f5403165668>),\n",
       " ('4', <pyspark.resultiterable.ResultIterable at 0x7f5403165f98>),\n",
       " ('2', <pyspark.resultiterable.ResultIterable at 0x7f5403165860>),\n",
       " ('3', <pyspark.resultiterable.ResultIterable at 0x7f5403165fd0>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 0.5213734076219673),\n",
       " ('4', 0.39765580325554173),\n",
       " ('2', 0.2739381988891164),\n",
       " ('3', 0.3976558032555418)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 2\n",
    "1 3\n",
    "2 3\n",
    "3 4\n",
    "4 1\n",
    "2 1\n",
    "\n",
    "groupByKey()\n",
    "links\n",
    "( (1, (2, 3) ),\n",
    "  (2, (3, 1) ),\n",
    "  (3, (4)    ),\n",
    "  (4, (1)    ),\n",
    ")\n",
    "ranks\n",
    "( (1, 1),\n",
    "  (2, 1),\n",
    "  (3, 1),\n",
    "  (4, 1)\n",
    ")\n",
    "\n",
    "join() ->\n",
    "( 1, (  (2, 3) , 1)\n",
    "...\n",
    "flatMap\n",
    "(2, 1/2), (3, 1/2), ....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join vs. Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, ((134, 'OK'), 'Apple')), (1, ((135, 'OK'), 'Apple')), (1, ((45, 'OK'), 'Apple')), (2, ((53, 'OK'), 'Orange')), (3, ((34, 'OK'), 'TV')), (5, ((162, 'Error'), 'Computer'))]\n"
     ]
    }
   ],
   "source": [
    "products = sc.parallelize([(1, \"Apple\"), (2, \"Orange\"), (3, \"TV\"), (5, \"Computer\")])\n",
    "trans = sc.parallelize([(1, (134, \"OK\")), (3, (34, \"OK\")), (5, (162, \"Error\")), (1, (135, \"OK\")), (2, (53, \"OK\")), (1, (45, \"OK\"))])\n",
    "\n",
    "print (trans.join(products).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join() --- Hash join**\n",
    "\n",
    "Use hash function to the both RDD, and send to the $hash-value^{th}$ worker. \n",
    "\n",
    "Two unbalanced size of RDD using hash join will be very inefficient! ( 100G / 10M )\n",
    "\n",
    "We can use **broadcase join** to increase the efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Apple', (134, 'OK')), (3, 'TV', (34, 'OK')), (5, 'Computer', (162, 'Error')), (1, 'Apple', (135, 'OK')), (2, 'Orange', (53, 'OK')), (1, 'Apple', (45, 'OK'))]\n"
     ]
    }
   ],
   "source": [
    "# products < trans\n",
    "products = {1: \"Apple\", 2: \"Orange\", 3: \"TV\", 5: \"Computer\"}\n",
    "trans = sc.parallelize([(1, (134, \"OK\")), (3, (34, \"OK\")), (5, (162, \"Error\")), (1, (135, \"OK\")), (2, (53, \"OK\")), (1, (45, \"OK\"))])\n",
    "\n",
    "broadcasted_products = sc.broadcast(products)\n",
    "\n",
    "results = trans.map(lambda x: (x[0], broadcasted_products.value[x[0]], x[1]))\n",
    "#  results = trans.map(lambda x: (x[0], products[x[0]], x[1]))\n",
    "print (results.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}